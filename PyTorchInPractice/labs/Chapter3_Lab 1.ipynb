{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d922f0a",
   "metadata": {
    "id": "7d922f0a"
   },
   "source": [
    "## 3.3 Lab 1 / Case 1: Non-Linear Regression\n",
    "\n",
    "In this lab, you will use the same [Auto MPG Dataset](https://archive.ics.uci.edu/ml/datasets/auto+mpg), but we'll bring more features to the mix, as you will also learn how to embed discrete/categorical features so they can be used to train the model.\n",
    "\n",
    "The columns, or attributes, of this dataset, are as follows:\n",
    "\n",
    "1. mpg: continuous\n",
    "2. cylinders: multi-valued discrete\n",
    "3. displacement: continuous\n",
    "4. horsepower: continuous\n",
    "5. weight: continuous\n",
    "6. acceleration: continuous\n",
    "7. model year: multi-valued discrete\n",
    "8. origin: multi-valued discrete\n",
    "9. car name: string (unique for each instance)\n",
    "\n",
    "Remember that the last column, `car name`, is actually separated by tabs (instead of spaces), so we're considering the cars' names as comments while loading the dataset.\n",
    "\n",
    "We're loading the dataset into a Pandas dataframe just like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f99d56",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "81f99d56",
    "outputId": "8c7b47a9-ba68-4e24-d853-8c366a62d28c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "column_names = ['mpg', 'cyl', 'disp', 'hp', 'weight', 'acc', 'year', 'origin']\n",
    "\n",
    "df = pd.read_csv(url, names=column_names, na_values='?', comment='\\t', sep=' ', skipinitialspace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3ac779",
   "metadata": {
    "id": "4c3ac779"
   },
   "source": [
    "### 3.3.1 Train-Validation-Test Split\n",
    "\n",
    "Split the dataset into train, validation, and test sets using Scikit-Learn's `train_test_split()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e720625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "raw_data = {}\n",
    "raw_data['train'], raw_data['test'] = ...\n",
    "raw_data['train'], raw_data['val'] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3604f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcabaa3f",
   "metadata": {
    "id": "dcabaa3f"
   },
   "source": [
    "### 3.3.2 Missing Values\n",
    "\n",
    "In this lab, we're throwing rows with missing values away, so make sure there are no NAs left in your datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb799c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fad9fe",
   "metadata": {
    "id": "69fad9fe"
   },
   "source": [
    "### 3.3.3 Continuous Attributes\n",
    "\n",
    "We've done this already, but this time you should write a `standardize()` function that:\n",
    "- takes a Pandas dataframe, a list of column names that are continuous attributes, and an optional scaler\n",
    "- creates and trains a Scikit-Learn's `StandardScaler` if one isn't provided as an argument\n",
    "- returns a PyTorch tensor containing the standardized features and an instance of Scikit-Learn's `StandardScaler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba2a130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def standardize(df, cont_attr, scaler=None):\n",
    "    # write your code here\n",
    "    ...\n",
    "    \n",
    "    return cont_X, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d06e9c",
   "metadata": {
    "id": "b1d06e9c"
   },
   "source": [
    "Use your `standardize` function to standardize all continuous attributes in our datasets. Don't forget you shouldn't train scalers on validation and test sets. They must use the scaler trained on the training set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126067ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_attr = ['mpg', 'disp', 'hp', 'weight', 'acc']\n",
    "\n",
    "cont_data = {'train': None, 'val': None, 'test': None}\n",
    "# write your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ed3bb0",
   "metadata": {
    "id": "46ed3bb0"
   },
   "source": [
    "### 3.3.4 Discrete and Categorical Attributes\n",
    "\n",
    "We've already talked about these attributes, but we didn't do anything about them. It is time to change that!\n",
    "\n",
    "Your goal here is to convert each possible value in a discrete or categorical attribute into a numerical array of a given length (that does not need to match the number of unique values). Before converting them into arrays, though, we need to encode them as sequential numbers first.\n",
    "\n",
    "Let's see what this looks like for the `cyl` attribute of our training dataset. It has only five unique vales: 3, 4, 5, 6, and 8 cylinders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f306496",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6f306496",
    "outputId": "60aa7157-a70d-46f9-a14b-796548155fd8"
   },
   "outputs": [],
   "source": [
    "cyls = sorted(raw_data['train']['cyl'].unique())\n",
    "cyls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b660d3",
   "metadata": {
    "id": "70b660d3"
   },
   "source": [
    "We can easily build a dictionary to map them into sequential numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WBykgnjl1X3H",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WBykgnjl1X3H",
    "outputId": "370de6f2-2cf2-4778-dc98-2af7c89b9ed0"
   },
   "outputs": [],
   "source": [
    "cyls_map = dict((v, i) for i, v in enumerate(cyls))\n",
    "cyls_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efeb6e8",
   "metadata": {
    "id": "2efeb6e8"
   },
   "source": [
    "Now imagine there's a lookup table with as many entries as unique values, each entry being a numerical array of a given length (say, eight elements). Let's create such a lookup table filled with random values as an illustration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760f488a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "760f488a",
    "outputId": "8c12848f-97e3-498a-99b1-fd7093bc5912"
   },
   "outputs": [],
   "source": [
    "n_dim = 8\n",
    "lookup_table = torch.randn((len(cyls), n_dim))\n",
    "lookup_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef56942c",
   "metadata": {
    "id": "ef56942c"
   },
   "source": [
    "There are five rows, each corresponding to a unique number of cylinders. Three cylinders, according to our mapping dictionary, corresponds to the first (index zero) row. Five cylinders, to the second (index one) row, and so on, and so forth.\n",
    "\n",
    "Let's say we'd like to retrieve the numerical array corresponding to six cylinders. We apply the mapping to find the corresponding index (`cyls_map[6]`) and use the result to actually slice the corresponding row from the lookup table (`lookup_table[idx]`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4e398a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0c4e398a",
    "outputId": "f511a608-669d-4873-f08f-2a70a39e8cb2"
   },
   "outputs": [],
   "source": [
    "idx = cyls_map[6]\n",
    "lookup_table[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628fbf57",
   "metadata": {
    "id": "628fbf57"
   },
   "source": [
    "There we go! Now, any number of cylinders can easily be mapped to a sequence of eight numerical values. It is as if any given number of cylinders, a categorical attribute, were now represented by eight numerical features instead. We have just (re)invented embeddings!\n",
    "\n",
    "The fact that these numbers are random is not necessarily an issue: we can simply turn the whole lookup table into parameters of the model itself, so they are also learned during training. The model will learn the best way to represent each value in categorical attribute as a sequence of numerical attributes! How cool is that?\n",
    "\n",
    "So, let's use PyTorch's `Embedding` layer instead of our own lookup table. The arguments are the same, though: the number of unique values, and the desired number of elements - or dimensions - in the returned numerical array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d2cbf0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a4d2cbf0",
    "outputId": "fee90919-0652-4b94-ef24-09ee63c2e2c8"
   },
   "outputs": [],
   "source": [
    "emb_table = nn.Embedding(len(cyls), n_dim)\n",
    "emb_table.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da36c436",
   "metadata": {
    "id": "da36c436"
   },
   "source": [
    "The embedding layer, like any other layer in PyTorch, is also a model. Its weights are, surprise, surprise, the lookup table itself. Besides, since it's a model, it can called as such and its expected input is a batch of indices. Let's try it out and see what we get out of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c50252b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4c50252b",
    "outputId": "a6e26207-daa4-4151-a18a-54fff66db535"
   },
   "outputs": [],
   "source": [
    "idx = cyls_map[6]\n",
    "emb_table(torch.as_tensor([idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d357c2",
   "metadata": {
    "id": "f5d357c2"
   },
   "source": [
    "There we go! Six cylinders is the fourth value in our encoded list, and therefore the embedding layer returned its fourth row of weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce07efea",
   "metadata": {
    "id": "ce07efea"
   },
   "source": [
    "A special case of embedding is the one-hot encoding (OHE) approach: instead of letting the model learn it during training, the mapping is fixed. In OHE, the numerical array has the same length as the number of unique values and it has only one nonzero element. It works as if each unique value were a dummy variable, for example: `cyl3`, `cyl4`, `cyl5`, `cyl6`, and `cyl8`, and only one of those dummy variables may have a nonzero value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c6bb23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "04c6bb23",
    "outputId": "1ee883be-637b-4e7d-f0b5-d76510c9cdaf"
   },
   "outputs": [],
   "source": [
    "ohe_table = torch.eye(len(cyls))\n",
    "ohe_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96644b4c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96644b4c",
    "outputId": "04634eec-93a8-4d11-cd7f-fabad78f2f42"
   },
   "outputs": [],
   "source": [
    "idx = cyls_map[6]\n",
    "ohe_table[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec97c93",
   "metadata": {
    "id": "9ec97c93"
   },
   "source": [
    "In this lab, we'll be using real, learnable, embeddings. Embeddings are an important part of modern deep learning, and a fundamental piece of natural language processing, as we'll see in Chapters 2 and 4.\n",
    "\n",
    "Now, it's your time to encode categorical attributes and create embeddings for them!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492d5d39",
   "metadata": {
    "id": "492d5d39"
   },
   "source": [
    "#### 3.3.4.1 Ordinal Encoder\n",
    "\n",
    "Instead of building dictionarie to manually encode categorical values into sequential numbers, write a function that uses Scikit-Learn's `OrdinalEncoder` instead. Similary to the standardization function you already wrote, this function:\n",
    "- takes a Pandas dataframe, a list of column names that are categorical attributes, and an optional encoder\n",
    "- creates and trains a Scikit-Learn's `OrdinalEncoder` if one isn't provided as an argument\n",
    "- returns a PyTorch tensor containing the encoded categorical features and an instance of Scikit-Learn's `OrdinalEncoder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce58a8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "def encode(df, cat_attr, encoder=None):\n",
    "    # write your code here\n",
    "    ...\n",
    "    \n",
    "    return cat_X, encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126ff94e",
   "metadata": {
    "id": "126ff94e"
   },
   "source": [
    "Use your `encode` function to encode all categorical attributes in our datasets. Don't forget you shouldn't train encoders on validation and test sets. They must use the encoder trained on the training set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9e557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_attr = ['cyl', 'origin']\n",
    "\n",
    "cat_data = {'train': None, 'val': None, 'test': None}\n",
    "# write your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c557c1",
   "metadata": {
    "id": "a8c557c1"
   },
   "source": [
    "The `categories_` attribute of the trained encoder should a list of lists of unique values, one list for each encoded attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2255b7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8f2255b7",
    "outputId": "f76ffc09-6f8e-4ac2-dee4-774bfdceeb8f"
   },
   "outputs": [],
   "source": [
    "encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612393dc",
   "metadata": {
    "id": "612393dc"
   },
   "source": [
    "If we check the encoded attributes, their unique values should be lists of sequential numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a04ce19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2a04ce19",
    "outputId": "5dcf58c2-b088-4d0d-d598-a4f2ba882059"
   },
   "outputs": [],
   "source": [
    "cat_data['train'][:, 0].unique(), cat_data['train'][:, 1].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1259b83e",
   "metadata": {
    "id": "1259b83e"
   },
   "source": [
    "### 3.3.5 Embeddings: From Categorical to Continuous\n",
    "\n",
    "Write code to create a list of embedding layers, each layer configured to handle one particular attribute, that is, one layer to embed `cyl` and another one to embed `origin`. You're free to choose the number of elements/dimensions that the resulting arrays will have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e268e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layers = []\n",
    "# write your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51331cbb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51331cbb",
    "outputId": "cc05481f-e5b1-4b8e-f0e0-1b136dc41954"
   },
   "outputs": [],
   "source": [
    "embedding_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92234516",
   "metadata": {
    "id": "92234516"
   },
   "source": [
    "Now, try out your layers by embedding the first five rows of your categorical training data. You should get a list containing two tensors with five rows and as many columns/dimensions as you choose in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3680021",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "# write your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317ac453",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "317ac453",
    "outputId": "35bcfdf3-120c-40a4-d90a-aa940777ba80"
   },
   "outputs": [],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65271f6f",
   "metadata": {
    "id": "65271f6f"
   },
   "source": [
    "In practice, thoug, your model won't be using a list of embeddings, but their concatenation along the horizontal axis instead. You can use `torch.cat` to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0474adff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0474adff",
    "outputId": "381ca666-0124-4822-8d25-e8e42963ef7c"
   },
   "outputs": [],
   "source": [
    "torch.cat(embeddings, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6881ae",
   "metadata": {
    "id": "3d6881ae"
   },
   "source": [
    "Now your categorical attributes are represented by many (learned) numerical features. Later on, when building your model, you will have to concatenate both the original continuous features, and those learned via embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92deaced",
   "metadata": {
    "id": "92deaced"
   },
   "source": [
    "### 3.3.6 Target and Task\n",
    "\n",
    "Your features are already taken care of, so it's time to create column tensors for your target attribute. Make sure they are of the type `float32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd90a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data = {'train': None, 'val': None, 'test': None}\n",
    "target_col = 'mpg'\n",
    "# write your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10723837",
   "metadata": {
    "id": "10723837"
   },
   "source": [
    "### 3.3.7 Custom Dataset\n",
    "\n",
    "Previously, we used a simple `TensorDataset` for our single feature and target. Now let's build our own custom dataset class instead by inheriting from the `Dataset` class. \n",
    "\n",
    "It needs to implement some basic methods:\n",
    "- `__init__(self)`\n",
    "- `__getitem__(self, index)`\n",
    "- `__len__(self)`. \n",
    "\n",
    "The constructor (`__init__()`) method may receive any arguments you can possible need, so you can create and preprocess your tensors right away or, as it is often the case when your dataset is too large, load them on demand. In our case, the constructor will receive the following arguments:\n",
    "\n",
    "- `raw_data`: a Pandas dataframe containing our (small) dataset\n",
    "- `cont_attr`: a list of the continuous attributes we'd like to use\n",
    "- `disc_attr`: a list of the discrete/categorical attributes we'd like to use\n",
    "- `target`: the name of the column containing the target attribute we'd like to predict\n",
    "- `scaler`: an optional instance of a `StandardScaler` to standardize the continuous attributes\n",
    "- `encoder`: an optional instance of an `OrdinalEncoder` to encode the discrete attributes sequentially\n",
    "\n",
    "You can use these arguments to preprocess and store the resulting tensors as class attributes, which you can retrieve at your convenience when other methods are called. Remember that you have already written functions to standardize continuous attributes and to encode categorical ones, feel free to use them.\n",
    "\n",
    "In the `__getitem__()` method, which makes a dataset \"sliceable\" just like a Python list, you should return a tuple `(features, target)` corresponding to the requested index. Notice that the first element of your tuple, `features` does not necessarily need to be a single tensor. It may be anything, another tuple, or even a dictionary. Remember that we have two types of features, continuous and categorical, and they are going to be handled differently in our model.\n",
    "\n",
    "In the `__len__()` method, you only need to return the total number of elements in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9656fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, raw_data, cont_attr, disc_attr, target_col, scaler=None, encoder=None):\n",
    "        self.n = ...\n",
    "        self.target = ...\n",
    "        self.cont_data, self.scaler = ...\n",
    "        self.cat_data, self.encoder = ...\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = ...\n",
    "        target = ...\n",
    "        return (features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e3a3c2",
   "metadata": {
    "id": "a9e3a3c2"
   },
   "source": [
    "Once your custom class has been defined, use it to create training, validation, and test datasets. Don't forget that scaling and encoding should be fitted in the training set only!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f06e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {'train': None, 'val': None, 'test': None}\n",
    "# write your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af92e523",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "af92e523",
    "outputId": "360f1740-9318-4513-e030-d4985c508e08"
   },
   "outputs": [],
   "source": [
    "datasets['train'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7919d44",
   "metadata": {
    "id": "d7919d44"
   },
   "source": [
    "You should see the features and targets of the first five elements from your training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89150a50",
   "metadata": {
    "id": "89150a50"
   },
   "source": [
    "### 3.3.8 Data Loaders\n",
    "\n",
    "Next, you need to create data loaders, one for each set. It is recommended to shuffle the training set, but don't bother shuffling the others. Dropping the last mini-batch, in case your set isn't a perfect multiple of your mini-batch size, is also recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ce4f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloaders = {'train': None, 'val': None, 'test': None}\n",
    "# write your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbf0b18",
   "metadata": {
    "id": "cfbf0b18"
   },
   "source": [
    "### 3.3.9 Custom Model\n",
    "\n",
    "Your next task is to build a custom model that can handle continuous and categorical features (via embeddings), and that is non-linear in nature. Before moving on, let's briefly discuss two topics: `ModuleList` and the importance of non-linearities.\n",
    "\n",
    "#### 3.3.9.1 `ModuleList`\n",
    "\n",
    "`ModuleList` is a special type of list, one that allows PyTorch to recursively look for learnable parameters of layers and model inside its contents. As it turns out, if the class attribute of your custom model is a regular Python list, any layers or models inside it will be ignore by PyTorch during training. By explicitly making a `ModuleList` out of a regular Python list we ensure that its parameters are also accounted for.\n",
    "\n",
    "In our custom model, we have a list of embedding layers, one for each categorical attribute. Therefore, if we want our model to properly learn these embeeddings, we need to make it a `ModuleList`.\n",
    "\n",
    "#### 3.3.9.2 Non-Linearities\n",
    "\n",
    "This is the \"secret sauce\" of deep learning models! If it weren't for non-linear activation functions, functions that twist and turn, or outright chop off, intermediate values, we would be eternally stuck with linear regression.\n",
    "\n",
    "If you line up two linear layers in a row with nothing in-between, these two linear layers will have an exact single-layer equivalent model. Only by adding non-linear activation functions between layers we can break this equivalence and make the model effectively more-complex.\n",
    "\n",
    "```python\n",
    "# Redundant\n",
    "model = nn.Sequential([nn.Linear(1, 10),\n",
    "                       nn.Linear(10, 1)])\n",
    "\n",
    "# Good!\n",
    "model = nn.Sequential([nn.Linear(1, 10),\n",
    "                       nn.ReLU(),  # non-linearity FTW!\n",
    "                       nn.Linear(10, 1)])\n",
    "```\n",
    "\n",
    "The use of non-linear activation functions is what allow models to learn complex decision boundaries when separating data points in different classes. For example, in a binary classification problem, a linear model can only produce a decision boundary that is, well, linear (left plot). A more-complex model that includes a non-linear activation function, on the other hand, can produce, well, non-linear boundaries (right plot).\n",
    "\n",
    "|Linear  |Non-Linear  |\n",
    "|---|---|\n",
    "| ![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch3/linear_boundary.png) | ![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch3/non_linear_boundaries.png) |\n",
    "\n",
    "***\n",
    "\n",
    "PyTorch implements many non-linear activation functions.\n",
    "\n",
    "Classical functions:\n",
    "\n",
    "- [Sigmoid](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid): the first activation function, chosen because of its mathematical properties, it is rarely used in-between layers; it can be used to convert logits into probabilities for binary clasification tasks;\n",
    "- [Tanh](https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html#torch.nn.Tanh): the hyperbolic tangent activation function was developed to overcome the major issue with sigmoid functions, the fact that it wasn't centered at zero; it is also rarely used in-between layers, but it's an internal component of other layers, such as recurrent layers.\n",
    "\n",
    "ReLU-family of functions:\n",
    "\n",
    "- [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU): the rectified linear unit is a simple yet powerful activation function as it simply preserves positive values while turning all negative values into zero; it addressed the problem of vanishing gradients (that is, when a model stops learning) and it spawned a whole family of activation functions;\n",
    "- [ReLU6](https://pytorch.org/docs/stable/generated/torch.nn.ReLU6.html#torch.nn.ReLU6): a ReLU function that is capped at six;\n",
    "- [LeakyReLU](https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html#torch.nn.LeakyReLU): the leaky rectified linear unit is a modified ReLU where negative values are multiplied by a tiny factor such as 0.01 instead of being turned into zero; it addressed the problem of \"dead neurons\" (that is, a neuron whose inputs are consistently negative and therefore does not have its weights updated);\n",
    "- [PReLU](https://pytorch.org/docs/stable/generated/torch.nn.PReLU.html#torch.nn.PReLU): the parametric version of the LeakyReLU, where the multiplying factor is also learned by the model.\n",
    "- [RReLU](https://pytorch.org/docs/stable/generated/torch.nn.RReLU.html#torch.nn.RReLU): randomized leaky rectified linear unit;\n",
    "- [SELU](https://pytorch.org/docs/stable/generated/torch.nn.SELU.html#torch.nn.SELU): scaled exponential linear unit;\n",
    "- [CELU](https://pytorch.org/docs/stable/generated/torch.nn.CELU.html#torch.nn.CELU): continuously differentiable exponential linear unit;\n",
    "- [GELU](https://pytorch.org/docs/stable/generated/torch.nn.GELU.html#torch.nn.GELU): Gaussian error linear unit;\n",
    "- [SiLU](https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html#torch.nn.SiLU): sigmoid linear unit;\n",
    "- [ELU](https://pytorch.org/docs/stable/generated/torch.nn.ELU.html#torch.nn.ELU): exponential linear unit.\n",
    "\n",
    "And more, less-known, functions as well. For a full list of activation functions, check the [Non-linear Activations](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity) section of PyTorch's documentation. Moreover, non-linearities are also available in [functional](https://pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions) form.\n",
    "\n",
    "For a more detailed explanation of the inner workings of activation functions, check my blog post [\"Hyper-parameters in Action - Part I: Activation Functions.\"](https://towardsdatascience.com/hyper-parameters-in-action-a524bf5bf1c)\n",
    "***\n",
    "\n",
    "#### 3.3.9.3 Methods\n",
    "\n",
    "A custom model class must implement a couple of methods:\n",
    "- `__init__(self)`\n",
    "- `forward(self, x)`\n",
    "\n",
    "In the constructor method, you will define the parts that make up your model, like linear layers and embeddings, as class attributes. Don't forget to include a call to `super().__init__()` at the top of the method so it executes the code from the parent class before your own. In our case, the model will receive the following arguments:\n",
    "\n",
    "- `n_cont`: the number of continuous attributes\n",
    "- `cat_list`: a list of lists of unique values of categorical attributes (as returned by the `categories_` property of the `OrdinalEncoder`)\n",
    "- `emb_dim`: the number of dimensions of each embedding (we're keeping them the same for every categorical attribute for simplicity)\n",
    "\n",
    "The `forward()` method is where the magic happens, as you know. It receives an input `x`, which can be anything (e.g. a tensor, a tuple, a dictionary), and forwards this input through your model's components, such as layers, activation functions, and embeddings. In the end, it should return a prediction. The diagram below illustrates the flow of the inputs through the model's components in the forward pass. Please refer to it for its implementation.\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch3/lab1_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b6df41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, n_cont, cat_list, emb_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layers\n",
    "        embedding_layers = []\n",
    "        # Creates one embedding layer for each categorical feature\n",
    "        \n",
    "        # write your code here\n",
    "        ...\n",
    "        \n",
    "        self.emb_layers = nn.ModuleList(embedding_layers)\n",
    "\n",
    "        # Total number of embedding dimensions\n",
    "        self.n_emb = len(cat_list) * emb_dim\n",
    "        self.n_cont = n_cont\n",
    "\n",
    "        # Linear Layer(s)\n",
    "        lin_layers = []\n",
    "        # The input layers takes as many inputs as the number of continuous features\n",
    "        # plus the total number of concatenated embeddings\n",
    "        # The number of outputs is your own choice\n",
    "        # Optionally, add more hidden layers, don't forget to match the dimensions if you do\n",
    "\n",
    "        # write your code here\n",
    "        ...\n",
    "        \n",
    "        self.lin_layers = nn.ModuleList(lin_layers)\n",
    "        \n",
    "        # The output layer must have as many inputs as there were outputs in the last hidden layer\n",
    "        self.output_layer = ...\n",
    "\n",
    "        # Layer initialization\n",
    "        for lin_layer in self.lin_layers:\n",
    "            nn.init.kaiming_normal_(lin_layer.weight.data, nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.output_layer.weight.data, nonlinearity='relu')\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # The inputs are the features as returned in the first element of a tuple\n",
    "        # coming from the dataset/dataloader\n",
    "        # Make sure you split it into continuous and categorical attributes according\n",
    "        # to your dataset implementation of __getitem__\n",
    "        cont_data, cat_data = inputs\n",
    "        \n",
    "        # Retrieve embeddings for each categorical attribute and concatenate them\n",
    "        embeddings = []\n",
    "        \n",
    "        # write your code here\n",
    "        ...\n",
    "        \n",
    "        embeddings = torch.cat(embeddings, 1)\n",
    "        \n",
    "        # Concatenate all features together, continuous and embeddings\n",
    "        x = ...\n",
    "        \n",
    "        # Run the inputs through each layer and applies an activation function to each output\n",
    "        for layer in self.lin_layers:\n",
    "            # write your code here\n",
    "            ...\n",
    "            \n",
    "        # Run the output of the last linear layer through the output layer\n",
    "        # write your code here\n",
    "        ...\n",
    "        \n",
    "        # Return the prediction\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d746fd17",
   "metadata": {},
   "source": [
    "Perhaps you noticed something unexpected in the constructor method of the model above, a couple of calls to something named `kaiming_normal_()`  layer initialization. We always start with random weights whenever we instantiate a new model, but we can tweak their initial distribution a little bit so it is less likely to end up in a vanishing gradients situation, that is, to stop learning altogether. Kaiming (also known as He) is the prescribed initialization to be used with ReLU activation functions. Initialization schemes may be relevant for training models from scratch, but since we'll be mostly using pretrained models, we won't be going into further details here. For a thorough explanation of the inner workings of different schemes, please check my blog post [\"Hyper-parameters in Action! Part II â€” Weight Initializers.\"](https://towardsdatascience.com/hyper-parameters-in-action-part-ii-weight-initializers-35aee1a28404)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce91b059",
   "metadata": {
    "id": "ce91b059"
   },
   "source": [
    "### 3.3.10 Training\n",
    "\n",
    "Now it is time to write your own training loop. \n",
    "\n",
    "First, you need to instantiate your model, create an optimizer for its parameters, and the appropriate loss function for the task. Use the data loaders to iterate through your training and validation data. If your features are a more-complex type (e.g. tuples or dictionaries), don't forget to send each one of its components to the appropriate device. Remember that model's have two modes, training and evaluation, set them accordingly. Optionally, you can also implement early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f99cb57",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1f99cb57",
    "outputId": "02cea2f4-f4f1-4594-9ec4-8e12c06e6680"
   },
   "outputs": [],
   "source": [
    "n_cont = scaler.n_features_in_\n",
    "cat_list = encoder.categories_\n",
    "\n",
    "n_cont, cat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb530a2",
   "metadata": {
    "id": "8eb530a2"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model = ...\n",
    "lr = 1e-2\n",
    "optimizer = ...\n",
    "loss_fn = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65e2cec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b65e2cec",
    "outputId": "b6a77ffd-2835-41c3-950b-8d2b4494ed75"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "losses = torch.empty(n_epochs)\n",
    "val_losses = torch.empty(n_epochs)\n",
    "\n",
    "best_loss = torch.inf\n",
    "best_epoch = -1\n",
    "patience = 3\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "progress_bar = tqdm(range(n_epochs))\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    batch_losses = torch.empty(len(dataloaders['train']))\n",
    "    \n",
    "    ## Training\n",
    "    for i, (batch_features, batch_targets) in enumerate(dataloaders['train']):\n",
    "        # Set the model to training mode\n",
    "        # write your code here\n",
    "        ...\n",
    "        \n",
    "        # Send batch features and targets to the device\n",
    "        # write your code here\n",
    "        ...\n",
    "        \n",
    "        # Step 1 - forward pass\n",
    "        predictions = ...\n",
    "\n",
    "        # Step 2 - computing the loss\n",
    "        loss = ...\n",
    "\n",
    "        # Step 3 - computing the gradients\n",
    "        # Tip: it requires a single method call to backpropagate gradients\n",
    "        # write your code here\n",
    "        ...\n",
    "        \n",
    "        batch_losses[i] = loss.item()\n",
    "\n",
    "        # Step 4 - updating parameters and zeroing gradients\n",
    "        # Tip: it takes two calls to optimizer's methods\n",
    "        # write your code here\n",
    "        ...\n",
    "        \n",
    "    losses[epoch] = batch_losses.mean()\n",
    "\n",
    "    ## Validation   \n",
    "    with torch.inference_mode():\n",
    "        batch_losses = torch.empty(len(dataloaders['val']))    \n",
    "\n",
    "        for i, (val_features, val_targets) in enumerate(dataloaders['val']):\n",
    "            # Set the model to evaluation mode\n",
    "            # write your code here\n",
    "            ...\n",
    "\n",
    "            # Send batch features and targets to the device\n",
    "            # write your code here\n",
    "            ...\n",
    "\n",
    "            # Step 1 - forward pass\n",
    "            predictions = ...\n",
    "\n",
    "            # Step 2 - computing the loss\n",
    "            loss = ...\n",
    "            \n",
    "            batch_losses[i] = loss.item()\n",
    "\n",
    "        val_losses[epoch] = batch_losses.mean()\n",
    "        \n",
    "        if val_losses[epoch] < best_loss:\n",
    "            best_loss = val_losses[epoch]\n",
    "            best_epoch = epoch\n",
    "            save_checkpoint(model, optimizer, \"best_model.pth\")\n",
    "        elif (epoch - best_epoch) > patience:\n",
    "            print(f\"Early stopping at epoch #{epoch}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63e8eb6",
   "metadata": {
    "id": "e63e8eb6"
   },
   "source": [
    "Let's check the evolution of the losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f28dc0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "15f28dc0",
    "outputId": "7586ba0d-8794-4823-da8d-49b53fee1a6f"
   },
   "outputs": [],
   "source": [
    "plt.plot(losses[:epoch], label='Training')\n",
    "plt.plot(val_losses[:epoch], label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c00b48e",
   "metadata": {
    "id": "4c00b48e"
   },
   "source": [
    "Then, let's compare predicted and actual values in the validation set. Hopefully, it will be much better than our former linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a8c2e1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "id": "e7a8c2e1",
    "outputId": "270a686a-3493-4ad2-ea01-c5335fdf16bb"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "split = 'val'\n",
    "batch = list(datasets[split][:][0])\n",
    "batch[0] = batch[0].to(device)\n",
    "batch[1] = batch[1].to(device)\n",
    "ax.scatter(datasets[split][:][1].tolist(), model(batch).tolist(), alpha=.5)\n",
    "#ax.scatter(y_true, y_hat)\n",
    "ax.plot([0, 45], [0, 45], linestyle='--', c='k', linewidth=1)\n",
    "ax.set_xlabel('Actual')\n",
    "ax.set_xlim([0, 45])\n",
    "ax.set_ylabel('Predicted')\n",
    "ax.set_ylim([0, 45])\n",
    "ax.set_title('MPG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90ba6d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
