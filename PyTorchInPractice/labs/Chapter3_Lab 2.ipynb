{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd8d017e",
   "metadata": {
    "id": "bd8d017e"
   },
   "source": [
    "## 3.4 Lab 2 / Case 2: Price Prediction\n",
    "\n",
    "In this we'll use a different dataset: [100,000 UK Used Car Data set](https://www.kaggle.com/datasets/adityadesai13/used-car-dataset-ford-and-mercedes) from Kaggle. It contains scraped data of used car listings split into CSV files according to the manufacturer: Audi, BMW, Ford, Hyundai, Mercedes, Skoda, Toyota, Vauxhall, and VW. It also contains a few extra files of particular models (`cclass.csv`, `focus.csv`, `unclean_cclass.csv`, and `unclean_focus.csv`) that we won't be using.\n",
    "\n",
    "Each file has nine columns with the car's attributes: model, year, price, transmission, mileage, fuel type, road tax, fuel consumption (mpg), and engine size. Transmission, fuel type, and year are discrete/categorical attributes, the others are continous. Our goal here is to predict the car's price based on its other attributes.\n",
    "\n",
    "We'll start by building a datapipe that reads all the information from the CSV files, and then we'll use this datapipe as a drop-in replacement for the dataset we typically use with data loaders. The use of datapipes in its functional form, will illustrate some of the challenges when dealing with real-world data.\n",
    "\n",
    "To download the dataset, you'll need to create a Kaggle account. In the following sections, we're assuming the dataset was downloaded and unzipped to a local folder named `car_prices`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca4ebe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://github.com/dvgodoy/assets/raw/main/PyTorchInPractice/data/100KUsedCar/car_prices.zip\n",
    "#!unzip car_prices.zip -d car_prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab61477",
   "metadata": {
    "id": "2ab61477"
   },
   "source": [
    "### 3.4.1 DataPipes\n",
    "\n",
    "There are many different available classes of data pipes in PyTorch. They are highly configurable, but we're sticking to the basics to illustrate how they work. The recommendation from PyTorch's team is to use its functional form and chain several operations in a sequence. We'll be doing exactly that, but we'll also be inspecting the results at the end of each operation to more easily understand what's happening under the hood.\n",
    "\n",
    "Our goal is to build a datapipe that produces a dictionary with three keys in it: `label` (containing the prices we want to predict), `cont_X` (an array of the continuous attributes), and `cat_X` (an array of sequentially-encoded categorical attributes).\n",
    "\n",
    "Let's start with a `FileLister` datapipe which, as its name says, lists all files inside a given folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f399cb",
   "metadata": {
    "id": "38f399cb"
   },
   "outputs": [],
   "source": [
    "import torchdata.datapipes as dp\n",
    "\n",
    "datapipe = dp.iter.FileLister('./car_prices')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d1ccae",
   "metadata": {
    "id": "40d1ccae"
   },
   "source": [
    "Unlike datasets, datapipes do not behave like lists, they do not implement the `__getitem__()` method. So, let's create a temporary dataloader in order to retrieve a couple of elements from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66383ad5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "66383ad5",
    "outputId": "05e945c2-3dd4-4102-a9eb-82bffbf803a7"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "next(iter(DataLoader(dataset=datapipe, batch_size=16)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cef92bc",
   "metadata": {
    "id": "3cef92bc"
   },
   "source": [
    "As expected, it listed every file (up to 16) inside that folder. But we don't actually want to use all of these files, we need to filter out some of them. Following the functional approach, we can call the `filter()` method of the datapipe we just created using a function that returns `True` only if the filename matches our filter conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fe0fee",
   "metadata": {
    "id": "98fe0fee"
   },
   "outputs": [],
   "source": [
    "def filter_for_data(filename):\n",
    "    return (\"unclean\" not in filename) and (\"focus\" not in filename) and (\"cclass\" not in filename) and filename.endswith(\".csv\")\n",
    "\n",
    "datapipe = datapipe.filter(filter_fn=filter_for_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bc8cec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76bc8cec",
    "outputId": "9d839089-6297-417d-dc18-86786ed2ef4b"
   },
   "outputs": [],
   "source": [
    "next(iter(DataLoader(dataset=datapipe, batch_size=16)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b14c13f",
   "metadata": {
    "id": "8b14c13f"
   },
   "source": [
    "Great, now we only have nine filenames, one for each manufacturer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef5bd1b",
   "metadata": {
    "id": "8ef5bd1b"
   },
   "source": [
    "#### 3.4.1.1 Loading CSV Files\n",
    "\n",
    "Next, we need to actually open and parse these CSV files. We're skipping the first line (it contains the headers), and we set its `return_path` to `True`, so we know which file each row came from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4470ea3",
   "metadata": {
    "id": "e4470ea3"
   },
   "outputs": [],
   "source": [
    "datapipe = datapipe.open_files(mode='rt')\n",
    "datapipe = datapipe.parse_csv(delimiter=\",\", skip_lines=1, return_path=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f11f93",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "27f11f93",
    "outputId": "f69e606e-a437-4ea1-cb87-59ce386a9fc4"
   },
   "outputs": [],
   "source": [
    "next(iter(DataLoader(dataset=datapipe, batch_size=4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76338589",
   "metadata": {
    "id": "76338589"
   },
   "source": [
    "Let's take a moment to analyze the output above. It is a list containing two elements:\n",
    "- a tuple containing four filenames\n",
    "- a list of nine tuples, each tuple containing four values\n",
    "\n",
    "Each tuple, both the first one and those inside the inner list, has four elements since we requested a mini-batch of four. The inner list has nine elements because there are nine columns in each CSV file. \n",
    "\n",
    "In effect, every element of our datapipe is a tuple `(filepath, features)`, but each feature is in its own tuple: `(filepath, [(f1,),(f2,), ...])`\n",
    "\n",
    "It would be interesting to have the manufacturer as a feature instead of a filename, right? Let's do that by calling the `map()` method of our datapipe with our own `get_manufacturer()` function. This function works by extracting the name of the manufacturer out of the filename, and appending it to the existing list of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb5a18a",
   "metadata": {
    "id": "9bb5a18a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_manufacturer(content):\n",
    "    path, data = content\n",
    "    manuf = os.path.splitext(os.path.basename(path))[0].upper()\n",
    "    data.extend([manuf])\n",
    "    return data\n",
    "\n",
    "datapipe = datapipe.map(get_manufacturer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd8a0a9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9bd8a0a9",
    "outputId": "c8564e7b-e8ba-4049-bfea-b5ed7e0bfdef"
   },
   "outputs": [],
   "source": [
    "next(iter(DataLoader(dataset=datapipe, batch_size=4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe6cd06",
   "metadata": {
    "id": "2fe6cd06"
   },
   "source": [
    "There are ten features now, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4245cef",
   "metadata": {
    "id": "d4245cef"
   },
   "source": [
    "#### 3.4.1.2 Encoding Categorical Attributes\n",
    "\n",
    "Now, it is time to encode categorical attributes, just like we did before using Scikit-Learn's `OrdinalEncoder`. There is an issue, though. We haven't even split our data into training, validation, and test sets yet. Moreover, it would be impractical to train an encoder inside a datapipe.\n",
    "\n",
    "So, let's take a step back and imagine that we had already discussed the problem at length, and we're aware of all valid unique values for each categorical attribute. It makes sense: if we're building an app that estimates car's prices, we'll eventually ask the end user to provide the characteristics of their car, most likely using dropdowns so they can choose from a list of predefined values. For example, you shouldn't let the user enter some made-up fuel type, they must choose among \"petrol\", \"diesel\", \"hybrid\", \"other\", or \"electric\". Each item in the dropdown corresponds to an integer value, so \"petrol\" is zero, \"diesel\" is one, and so on. That's the same as sequentially-encoding the fuel type.\n",
    "\n",
    "Of course, we won't be actually designing the frontend of an app in this lab, so let's just pretend we did it, and cheat a little bit by looking at the whole data first and building dictionaries that perform the encoding described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7175bc3",
   "metadata": {
    "id": "a7175bc3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "colnames = ['model', 'year', 'price', 'transmission', 'mileage', 'fuel_type', 'road_tax', 'mpg', 'engine_size', 'manufacturer']\n",
    "df = pd.DataFrame(list(datapipe), columns=colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32525cf7",
   "metadata": {
    "id": "32525cf7"
   },
   "outputs": [],
   "source": [
    "N_ROWS = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c938c6f3",
   "metadata": {
    "id": "c938c6f3"
   },
   "outputs": [],
   "source": [
    "def gen_encoder_dict(series):\n",
    "    values = series.unique()\n",
    "    return dict(zip(values, range(len(values))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3203638",
   "metadata": {
    "id": "c3203638"
   },
   "outputs": [],
   "source": [
    "cont_attr = ['year', 'mileage', 'road_tax', 'mpg', 'engine_size']\n",
    "cat_attr = ['model', 'transmission', 'fuel_type', 'manufacturer']\n",
    "\n",
    "dropdown_encoders = {col: gen_encoder_dict(df[col]) for col in cat_attr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715fd236",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "715fd236",
    "outputId": "97736360-5fe7-4c1a-d199-49589bad2ada"
   },
   "outputs": [],
   "source": [
    "dropdown_encoders['fuel_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7525356d",
   "metadata": {
    "id": "7525356d"
   },
   "source": [
    "Datapipes are less flexible than datasets in this sense. They require that you know, beforehand, what your data structure is, what your data actually looks like, and what transformations are needed.\n",
    "\n",
    "So, let's use this knowledge to build a preprocessing function that takes a row as input - containing ten columns of data - and produces the desired dictionary as output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399c1848",
   "metadata": {
    "id": "399c1848"
   },
   "source": [
    "#### 3.4.1.3 Row Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee44eb8a",
   "metadata": {
    "id": "ee44eb8a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preproc(row):\n",
    "    colnames = ['model', 'year', 'price', 'transmission', 'mileage', 'fuel_type', 'road_tax', 'mpg', 'engine_size', 'manufacturer']\n",
    "    \n",
    "    cat_attr = ['model', 'transmission', 'fuel_type', 'manufacturer']\n",
    "    cont_attr = ['year', 'mileage', 'road_tax', 'mpg', 'engine_size']\n",
    "    target = 'price'\n",
    "    \n",
    "    vals = dict(zip(colnames, row))\n",
    "    cont_X = [float(vals[name]) for name in cont_attr]\n",
    "    cat_X = [dropdown_encoders[name][vals[name]] for name in cat_attr]\n",
    "            \n",
    "    return {'label': np.array([float(vals[target])], dtype=np.float32),\n",
    "            'cont_X': np.array(cont_X, dtype=np.float32), \n",
    "            'cat_X': np.array(cat_X, dtype=int)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef290b8a",
   "metadata": {
    "id": "ef290b8a"
   },
   "source": [
    "We can, once again, use the datapipe's `map()` method to apply the function above to every data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56f5a06",
   "metadata": {
    "id": "e56f5a06"
   },
   "outputs": [],
   "source": [
    "datapipe = datapipe.map(preproc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1626cb76",
   "metadata": {
    "id": "1626cb76"
   },
   "source": [
    "Let's take a mini-batch of four data points from our datapipe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd931dad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cd931dad",
    "outputId": "172c7509-274f-4c34-fe2f-9ca1143574b0"
   },
   "outputs": [],
   "source": [
    "next(iter(DataLoader(dataset=datapipe, batch_size=4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c606d0a8",
   "metadata": {
    "id": "c606d0a8"
   },
   "source": [
    "Nice! We got the desired dictionary back, each key has a tensor with four rows (our mini-batch size), and the categorical attributes are encoded as integers.\n",
    "\n",
    "At this point, you're probably wondering why we didn't bother at all to standardize/scale the continuous attributes. Don't worry, we'll get back to it in a couple of sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb9421f",
   "metadata": {
    "id": "6cb9421f"
   },
   "source": [
    "#### 3.4.1.4 The Full DataPipe and Splits\n",
    "\n",
    "Let's piece all the parts together and re-build the full datapipe from top to bottom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afed88a",
   "metadata": {
    "id": "4afed88a"
   },
   "outputs": [],
   "source": [
    "datapipe = dp.iter.FileLister('./car_prices')\n",
    "datapipe = datapipe.filter(filter_fn=filter_for_data)\n",
    "datapipe = datapipe.open_files(mode='rt')\n",
    "datapipe = datapipe.parse_csv(delimiter=\",\", skip_lines=1, return_path=True)\n",
    "datapipe = datapipe.map(get_manufacturer)\n",
    "datapipe = datapipe.map(preproc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b205be77",
   "metadata": {
    "id": "b205be77"
   },
   "source": [
    "The datapipe is ready, and it works like a \"recipe\" of all the steps required to load, clean, and preprocess our data. Now, let's call its `random_split()` method to create the training, validation, and test sets. We shouldn't forget to shuffle the training set, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ee0a98",
   "metadata": {
    "id": "40ee0a98"
   },
   "outputs": [],
   "source": [
    "datapipes = {}\n",
    "datapipes['train'] = datapipe.random_split(total_length=N_ROWS, weights={\"train\": 0.8, \"val\": 0.1, \"test\": 0.1}, seed=11, target='train')\n",
    "datapipes['val'] = datapipe.random_split(total_length=N_ROWS, weights={\"train\": 0.8, \"val\": 0.1, \"test\": 0.1}, seed=11, target='val')\n",
    "datapipes['test'] = datapipe.random_split(total_length=N_ROWS, weights={\"train\": 0.8, \"val\": 0.1, \"test\": 0.1}, seed=11, target='test')\n",
    "\n",
    "datapipes['train'] = datapipes['train'].shuffle(buffer_size=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dcc2a0",
   "metadata": {
    "id": "04dcc2a0"
   },
   "source": [
    "Datapipes are a drop-in replacement for datasets, so we can simply use data loaders exactly the same way we've been doing so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2e8993",
   "metadata": {
    "id": "ac2e8993"
   },
   "outputs": [],
   "source": [
    "dataloaders = {}\n",
    "dataloaders['train'] = DataLoader(dataset=datapipes['train'], batch_size=128, drop_last=True, shuffle=True)\n",
    "dataloaders['val'] = DataLoader(dataset=datapipes['val'], batch_size=128)\n",
    "dataloaders['test'] = DataLoader(dataset=datapipes['test'], batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c0da42",
   "metadata": {
    "id": "56c0da42"
   },
   "source": [
    "### 3.4.2 BatchNorm for Continuous Attributes\n",
    "\n",
    "As promised a couple of sections ago, let's discuss what to do with the continuous attributes. Just like with the ordinal encoder, it isn't practical to train a `StandardScaler` inside a datapipe. Moreover, how are we supposed to fit a `StandardScaler` on training data only, if the split is performed at the very end of the datapipe?\n",
    "\n",
    "Luckily, we don't necessarily need to standardize the data using statistics (mean and standard deviation) computed on the whole training set. We can standardize them using running mini-batch statistics instead! That's what batch normalization does. \n",
    "\n",
    "Let's see how it works by, first, retrieving a mini-batch of data and computing the statistics of its continuous attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32ee8a8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c32ee8a8",
    "outputId": "83998ea5-e579-4c16-a1a1-ef023cb59c37"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "batch = next(iter(dataloaders['train']))\n",
    "batch['cont_X'].mean(axis=0), batch['cont_X'].std(axis=0, unbiased=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86239f28",
   "metadata": {
    "id": "86239f28"
   },
   "source": [
    "Now, let's create an instance of a batch norm layer, use our mini-batch as input, and compute statistics on the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55719029",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55719029",
    "outputId": "fac8f821-45ab-4e34-ffab-ff71e18f1ca5"
   },
   "outputs": [],
   "source": [
    "bn_layer = nn.BatchNorm1d(num_features=len(cont_attr))\n",
    "\n",
    "normalized_cont = bn_layer(batch['cont_X'])\n",
    "normalized_cont.mean(axis=0), normalized_cont.std(axis=0, unbiased=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ac4ed8",
   "metadata": {
    "id": "64ac4ed8"
   },
   "source": [
    "There we go! The continuous attributes of our mini-batch were standardized (or normalized, following technique's name) so they are zero-centered and have unit standard deviation. As it turns out, the batch normalization layer keeps track of running statistics, so after seeing this one mini-batch of data, it will have some statistics of its own already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c93ab15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6c93ab15",
    "outputId": "0012b347-a193-49e8-92d3-32deae45e22d"
   },
   "outputs": [],
   "source": [
    "bn_layer.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ade3c1",
   "metadata": {
    "id": "79ade3c1"
   },
   "source": [
    "When in training mode, it keeps updating running statistics so, after one epoch, it will have collected statistics over the whole training set. At this point, it will have statistics very close to those we would get if we had computed them over the whole training set in the first place.\n",
    "\n",
    "Once the model is switched to evaluation mode, the batch norm layer doesn't update its internal statistics anymore, but it still normalizes new data points using those it learned during training. Batch norm layers, together with dropout layers, are a classical example of having distinct behaviors depending on which mode the model was set to.\n",
    "\n",
    "All we have to do now is to add one of these layers to normalize the inputs of our model and, optionally, after every hidden layer as well. That may raise a question: should we place the batch normalization before or after the activation function? On a theoretical level, it makes more sense to place it after the activation function, so the outputs are zero-centered. However, successful models such as Inception V3 place it before the activation function. Unfortunately, there's no straight answer to this question, the choice is yours to make."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14b9812",
   "metadata": {
    "id": "d14b9812"
   },
   "source": [
    "### 3.4.3 Custom Model\n",
    "\n",
    "You know the drill: write a custom model class that implements both `__init__()` and `forward()` methods. You can use the model you wrote in Lab 1 as a starting point.\n",
    "\n",
    "In the constructor method, you will define the parts that make up your model, like linear layers and embeddings, as class attributes. Don't forget to include a call to `super().__init__()` at the top of the method so it executes the code from the parent class before your own. In our case, the model will receive the following arguments:\n",
    "\n",
    "- `n_cont`: the number of continuous attributes\n",
    "- `cat_list`: a list of lists of unique values of categorical attributes (as returned by the `categories_` property of the `OrdinalEncoder`)\n",
    "- `emb_dim`: the number of dimensions of each embedding (we're keeping them the same for every categorical attribute for simplicity)\n",
    "\n",
    "The `forward()` method is where the magic happens, as you know. It receives an input `x`, which can be anything (e.g. a tensor, a tuple, a dictionary), and forwards this input through your model's components, such as layers, activation functions, and embeddings. In the end, it should return a prediction.\n",
    "\n",
    "Don't forget your data loader is returning dictionaries now, you'll need to make adjustments to how your model treats its inputs. Also, don't forget to add a batch normalization layer to preprocess the continuous attributes and, optionally, you can also add batch normalization layers after each hidden linear layer. Please refer to the diagram below for the implementation.\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch3/lab2_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19e2712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, n_cont, cat_list, emb_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layers\n",
    "        embedding_layers = []\n",
    "        # Creates one embedding layer for each categorical feature\n",
    "\n",
    "        # write your code here\n",
    "        ...\n",
    "        self.emb_layers = ...\n",
    "\n",
    "        # Total number of embedding dimensions\n",
    "        self.n_emb = len(cat_list) * emb_dim\n",
    "        self.n_cont = n_cont\n",
    "        # Batch Normalization layer for continuous features\n",
    "        self.bn_input = nn.BatchNorm1d(n_cont)\n",
    "\n",
    "        # Linear Layer(s)\n",
    "        lin_layers = []\n",
    "        # The input layers takes as many inputs as the number of continuous features\n",
    "        # plus the total number of concatenated embeddings\n",
    "        # The number of outputs is your own choice\n",
    "        # Optionally, add more hidden layers, don't forget to match the dimensions if you do\n",
    "        \n",
    "        # write your code here\n",
    "        ...\n",
    "        self.lin_layers = ...\n",
    "\n",
    "        # Batch Normalization Layer(s)\n",
    "        bn_layers = []\n",
    "        # Creates batch normalization layers for each linear hidden layer\n",
    "\n",
    "        # write your code here\n",
    "        ...\n",
    "        self.bn_layers = ...\n",
    "        \n",
    "        # The output layer must have as many inputs as there were outputs in the last hidden layer\n",
    "        self.output_layer = ...\n",
    "\n",
    "        # Layer initialization\n",
    "        for lin_layer in self.lin_layers:\n",
    "            nn.init.kaiming_normal_(lin_layer.weight.data, nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.output_layer.weight.data, nonlinearity='relu')\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # The inputs are the features as returned in the first element of a tuple\n",
    "        # coming from the dataset/dataloader\n",
    "        # Make sure you split it into continuous and categorical attributes according\n",
    "        # to your dataset implementation of __getitem__\n",
    "        cont_data, cat_data = ...\n",
    "        \n",
    "        # Retrieve embeddings for each categorical attribute and concatenate them\n",
    "        embeddings = []\n",
    "        # write your code here\n",
    "        ...\n",
    "        \n",
    "        # Normalizes continuous features using Batch Normalization layer\n",
    "        normalized_cont_data = self.bn_input(cont_data)\n",
    "        \n",
    "        # Concatenate all features together, normalized continuous and embeddings\n",
    "        x = ...\n",
    "        \n",
    "        # Run the inputs through each layer and applies an activation function and batch norm to each output\n",
    "        for layer, bn_layer in zip(self.lin_layers, self.bn_layers):\n",
    "            # write your code here\n",
    "            ...\n",
    "            \n",
    "        # Run the output of the last linear layer through the output layer\n",
    "        ...\n",
    "        \n",
    "        # Return the prediction\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290c12c2",
   "metadata": {
    "id": "290c12c2"
   },
   "source": [
    "### 3.4.4 Training\n",
    "\n",
    "Now it is time to write your own training loop. Once again, you need to instantiate your model, create an optimizer for its parameters, and the appropriate loss function for the task. The training loop itself is pretty much the same as in the previous lab, but don't forget your data loaders return dictionaries now, so you'll need to adjust they way your data is being sent to the appropriate device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958ecd2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "958ecd2b",
    "outputId": "50a09a5f-9fa9-41a5-dee1-877971c36409"
   },
   "outputs": [],
   "source": [
    "n_cont = len(cont_attr)\n",
    "cat_list = [np.array(list(dropdown_encoders[name].values())) for name in cat_attr]\n",
    "\n",
    "n_cont, cat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf85efc7",
   "metadata": {
    "id": "bf85efc7"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "lr = 3e-3\n",
    "\n",
    "model = ...\n",
    "optimizer = ...\n",
    "loss_fn = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f583d8c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1f583d8c",
    "outputId": "5133e0de-1788-49c0-b207-3d760f5bfc68"
   },
   "outputs": [],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e465e31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9e465e31",
    "outputId": "a8291fa7-4e7f-4cce-b605-95015fab7813"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "n_epochs = 20\n",
    "\n",
    "losses = torch.empty(n_epochs)\n",
    "val_losses = torch.empty(n_epochs)\n",
    "\n",
    "best_loss = torch.inf\n",
    "best_epoch = -1\n",
    "patience = 3\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "progress_bar = tqdm(range(n_epochs))\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    batch_losses = []\n",
    "    \n",
    "    ## Training\n",
    "    for i, batch in enumerate(dataloaders['train']):\n",
    "        # Set the model to training mode\n",
    "        # write your code here\n",
    "        ...\n",
    "        \n",
    "        # Send batch features and targets to the device\n",
    "        # write your code here\n",
    "        ...\n",
    "        \n",
    "        # Step 1 - forward pass\n",
    "        predictions = ...\n",
    "\n",
    "        # Step 2 - computing the loss\n",
    "        loss = ...\n",
    "\n",
    "        # Step 3 - computing the gradients\n",
    "        # Tip: it requires a single method call to backpropagate gradients\n",
    "        # write your code here\n",
    "        ...\n",
    "\n",
    "        batch_losses.append(loss.item())\n",
    "\n",
    "        # Step 4 - updating parameters and zeroing gradients\n",
    "        # Tip: it takes two calls to optimizer's methods\n",
    "        # write your code here\n",
    "        ...\n",
    "        \n",
    "    losses[epoch] = torch.tensor(batch_losses).mean()\n",
    "\n",
    "    ## Validation   \n",
    "    with torch.inference_mode():\n",
    "        batch_losses = []\n",
    "\n",
    "        for i, val_batch in enumerate(dataloaders['val']):\n",
    "            # Set the model to evaluation mode\n",
    "            # write your code here\n",
    "            ...\n",
    "\n",
    "            # Send batch features and targets to the device\n",
    "            # write your code here\n",
    "            ...\n",
    "\n",
    "            # Step 1 - forward pass\n",
    "            predictions = ...\n",
    "\n",
    "            # Step 2 - computing the loss\n",
    "            loss = ...\n",
    "\n",
    "            batch_losses.append(loss.item())\n",
    "\n",
    "        val_losses[epoch] = torch.tensor(batch_losses).mean()\n",
    "        \n",
    "        if val_losses[epoch] < best_loss:\n",
    "            best_loss = val_losses[epoch]\n",
    "            best_epoch = epoch\n",
    "            save_checkpoint(model, optimizer, \"best_model.pth\")\n",
    "        elif (epoch - best_epoch) > patience:\n",
    "            print(f\"Early stopping at epoch #{epoch}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0f2bcb",
   "metadata": {
    "id": "6a0f2bcb"
   },
   "source": [
    "Let's check the evolution of the losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a0b5b1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "48a0b5b1",
    "outputId": "01751658-c39f-4fc8-f828-18eca51c0e2e"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses[:epoch], label='Training')\n",
    "plt.plot(val_losses[:epoch], label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f78468",
   "metadata": {
    "id": "62f78468"
   },
   "source": [
    "Then, let's compare predicted and actual values in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e556b858",
   "metadata": {
    "id": "e556b858"
   },
   "outputs": [],
   "source": [
    "split = 'val'\n",
    "y_hat = []\n",
    "y_true = []\n",
    "for batch in dataloaders[split]:\n",
    "    model.eval()\n",
    "    batch['cont_X'] = batch['cont_X'].to(device)\n",
    "    batch['cat_X'] = batch['cat_X'].to(device)\n",
    "    batch['label'] = batch['label'].to(device)\n",
    "    y_hat.extend(model(batch).tolist())\n",
    "    y_true.extend(batch['label'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18792059",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "id": "18792059",
    "outputId": "765b2213-796e-411e-c504-8bddad48731f"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "ax.scatter(y_true, y_hat, alpha=0.25)\n",
    "ax.plot([0, 80000], [0, 80000], linestyle='--', c='k', linewidth=1)\n",
    "ax.set_xlabel('Actual')\n",
    "ax.set_xlim([0, 80000])\n",
    "ax.set_ylabel('Predicted')\n",
    "ax.set_ylim([0, 80000])\n",
    "ax.set_title('Price')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c95fa2",
   "metadata": {
    "id": "58c95fa2"
   },
   "source": [
    "Ideally, you'll see a cloud of points around the diagonal line. What about the R2 score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3383bd06",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3383bd06",
    "outputId": "63795bfb-9ef0-4bb5-e4ee-d4e19cee4e35"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_true, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e67afc7",
   "metadata": {
    "id": "8e67afc7"
   },
   "source": [
    "If your cloud of points were indeed around the diagonal line, you're probably expecting a high R2 score (>0.8). If you got a surprisingly low value for it, can you guess why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4da6a79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
