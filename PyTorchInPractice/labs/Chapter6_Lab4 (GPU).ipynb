{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install portalocker sentencepiece sacremoses transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b17bded5"
   },
   "source": [
    "## 6.4 Lab 4 / Case 4: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4d1bb429"
   },
   "source": [
    "In this lab, you'll fine-tune an encoder-based model to perform sentiment analysis on the Standford Sentiment Treebank (SST2) dataset. You'll load RoBERTa's sibling, XLM-RoBERTa, use its prescribed transformations to preprocess text in the SST2 dataset, and fine-tune (train) it for one epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "585fd02c"
   },
   "source": [
    "### 6.4.1 Model\n",
    "\n",
    "You'll use Torchtext's `XLMR_BASE_ENCODER` in this lab. Create an instance of a classification head (`RobertaClassificationHead`) to perform binary classification (we have two classes, \"positive\" and \"negative\" sentiment), matching the input dimensions to the embeddings generated by the base model, and then load the model with the head attached to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fef7dee0",
    "outputId": "3fe45605-18c0-4e60-8f4c-207a06718e07"
   },
   "outputs": [],
   "source": [
    "import torchtext\n",
    "\n",
    "xlmr_base = torchtext.models.XLMR_BASE_ENCODER\n",
    "xlmr_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6453fb87",
    "outputId": "3ec2c9d6-fcb2-46d0-937e-f83bfc7d0468"
   },
   "outputs": [],
   "source": [
    "classifier_head = ...\n",
    "# Tip: you can call a method from xlmr_base to load the model with the head\n",
    "model = ...\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52f10c74"
   },
   "source": [
    "### 6.4.2 Dataset\n",
    "\n",
    "Now, you will load Torchtext's [\"Stanford Sentiment Treebank (SST2)\"](https://pytorch.org/text/stable/datasets.html#sst2) dataset. This dataset uses Torchdata's `DataPipe`s instead of traditional `Dataset`s. It is already split into `train`, `dev` (validation), and `test` sets. You only need to specify it in the `split` argument in the constructor of `SST2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65b37a60"
   },
   "outputs": [],
   "source": [
    "from torchtext.datasets import SST2\n",
    "\n",
    "datapipes = {}\n",
    "datapipes['train'] = ...\n",
    "datapipes['val'] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f933d20a"
   },
   "source": [
    "Let's take a look at one data point from the SST2 dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3d15fa20",
    "outputId": "f2707daa-10dc-4bc8-b583-e9a43db5339d"
   },
   "outputs": [],
   "source": [
    "row = next(iter(datapipes['train']))\n",
    "text, label = row\n",
    "text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "377fe053"
   },
   "source": [
    "Each data point is a tuple, containing a line of text, and the corresponding label - the sentiment (0 for negative, 1 for positive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6eebde7e"
   },
   "source": [
    "### 6.4.3 Transforms\n",
    "\n",
    "You already know the drill: you must preprocess the input (the text) using the prescribed transformation for the model you're using, so it gets tokenized, converted into token ids, and prependend/appended with the appropriate special tokens.\n",
    "\n",
    "Retrieve the transformation function/model from the XLM-RoBERTa model, and write a function that takes a tuple of `(text, label)` and returns another tuple of `(list of tokens ids, label)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0c217192",
    "outputId": "94cda470-7ed9-4584-9703-9a52a723cc3f"
   },
   "outputs": [],
   "source": [
    "transform_fn = ...\n",
    "transform_fn(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f18d864c"
   },
   "outputs": [],
   "source": [
    "def apply_transform(row):\n",
    "    text, label = row\n",
    "    # write your code here\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcd58738"
   },
   "source": [
    "Let's apply your function to our data point to see if it is working as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ca73f45",
    "outputId": "0d56f4f0-5e41-4fd5-9318-feb2b015dba2"
   },
   "outputs": [],
   "source": [
    "apply_transform(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32ec29d1"
   },
   "source": [
    "Did you notice the transformation is returning a regular Python list of token ids, not a PyTorch tensor? Remember, we cannot make a tensor out of lists of different lengths (see section 6.3.3). The solution? Padding the shorter sentences, so they all have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50064ea9"
   },
   "outputs": [],
   "source": [
    "padding_idx = transform_fn[1].vocab.lookup_indices(['<pad>'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3412fd5"
   },
   "source": [
    "We'll be padding the sentences and building tensors out of them during pipeline creation so it is more streamlined but you could also do it inside the training loop, after they're returned by the data loader and before they're sent as inputs to the model.\n",
    "\n",
    "Write a function that takes a batch of (transformed) data points, pads the sequences (using `to_tensor` and the padding id provided above), and converts the labels into a tensor as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38b91d89"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.functional import to_tensor\n",
    "\n",
    "def tensor_batch(batch):\n",
    "    tokens = batch['token_ids']\n",
    "    labels = batch['labels']\n",
    "    # write your code here\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74c05213"
   },
   "source": [
    "You're probably wondering: how could the pipeline pad the sequences if it transforming data points individually? Isn't the data loader's role to produce mini-batches? And, where did the dictionary come from?!\n",
    "\n",
    "Yes, it is usually the data loader's role to produce mini-batches, but it turns out we can make also batches inside the data pipe already, so we can pad the sequences appropriately.\n",
    "\n",
    "We'll be using two methods to accomplish this:\n",
    "- `batch()`: it takes the number of data points that will make up the mini-batch\n",
    "- `rows2columnar()`: it \"transposes\" the data so that the a list of tuples becomes a tuple of lists. For example, let's say we have two data points `(f1, l1)` and `(f2, l2)`. A mini-batch of two would be a list of tuples `[(f1, l1), (f2, l2)]` but if we make it columnar, it will become a tuple of lists `([f1, f2], [l1, l2])` or, better yet, a dictionary where the keys are the column names passed as arguments: `{col1: [f1, f2], 'col2: [l1, l2]}`.\n",
    "\n",
    "And that should answer the question \"where did the dictionary come from.\"\n",
    "\n",
    "Now, let's line up all these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50bded1a"
   },
   "outputs": [],
   "source": [
    "for k in datapipes.keys():\n",
    "    datapipes[k] = datapipes[k].map(apply_transform)\n",
    "    datapipes[k] = datapipes[k].batch(16)\n",
    "    datapipes[k] = datapipes[k].rows2columnar(['token_ids', 'labels'])\n",
    "    datapipes[k] = datapipes[k].map(tensor_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b77e025b"
   },
   "source": [
    "If we fetch from our data pipe, it should return a tuple of two tensors, each tensor containing as many rows as the mini-batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e870aca2",
    "outputId": "4e957dc6-1964-4487-aa69-d2bd0eb94a66"
   },
   "outputs": [],
   "source": [
    "dp_out = next(iter(datapipes['train']))\n",
    "dp_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0d570206"
   },
   "source": [
    "Now, create a data loader for each data pipe. Since the batches are already defined inside the data pipe, the batch size should be `None`. It is still OK to shuffle the training set, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "76c804f3"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloaders = {}\n",
    "dataloaders['train'] = ...\n",
    "dataloaders['val'] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "923f3846"
   },
   "source": [
    "Now, let's fetch a mini-batch from our data loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05babc84",
    "outputId": "270b0792-7d7a-44b1-a491-b404e7c3f739"
   },
   "outputs": [],
   "source": [
    "dl_out = next(iter(dataloaders['train']))\n",
    "dl_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4eb7806"
   },
   "source": [
    "Do you see any difference between the two outputs, from the (batched) datapipe and the data loader? The former returns a tuple while the latter returns a list, but the contents are the same: a mini-batch of features and a mini-batch of labels. The length of the features may differ depending on how long the longest sequence in a given mini-batch is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "093b38c7",
    "outputId": "0d5cba8f-f09e-46b8-ec5c-83a40aca5a1d"
   },
   "outputs": [],
   "source": [
    "dp_out[0].shape, dl_out[0].shape # features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b5e15e9f",
    "outputId": "3d2a2e36-9020-4f7e-e65a-aefc52354546"
   },
   "outputs": [],
   "source": [
    "dp_out[1].shape, dl_out[1].shape # labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0374eb63"
   },
   "source": [
    "This means that it is possible to use data pipes directly in the training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "171714e6"
   },
   "source": [
    "### 6.4.4 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "260d405e"
   },
   "source": [
    "Now, it is time to write a training loop to fine-tune your XLM-RoBERTa model on the SST2 dataset. This is a large model, and the training set has over 60,000 data points, so you can train it over a single epoch, that is, looping over the mini-batches from the datapipe (or data loader) only once. For the sake of speed, keep the evalution for the end only.\n",
    "\n",
    "Although `Adam` is the optimizer of choice, we suggest you try out `AdamW`, a modified version that is also commonly used.\n",
    "\n",
    "Sentiment analysis is a classification task, so we need to use the appropriate loss function for the task. Even though it is a binary classification, RoBERTa's classification head is actually producing two logits instead of one, so you have to use `CrossEntropyLoss` (which can handle two or more logits using softmax functio to convert them into probabilities).\n",
    "\n",
    "***\n",
    "\n",
    "**Classification Losses Showdown**\n",
    "\n",
    "Honestly, I always feel this whole thing is a bit confusing, especially for someone who's learning it for the first time. \n",
    "\n",
    "Which loss functions take logits as inputs? Should I add a (log)softmax layer or not? Can I use the `weight` argument to handle imbalanced datasets? Too many questions, right?\n",
    "\n",
    "So, here is a table to help you figure out the landscape of loss functions for classification problems, both binary and multiclass:\n",
    "\n",
    "|                         | BCE Loss               | BCE With Logits Loss     | NLL Loss                    | Cross-Entropy Loss   \n",
    "| --- | --- | --- | --- | --- |\n",
    "|     Classification      | binary                | binary                | multiclass                 | multiclass \n",
    "| Input (each data point) | probability           | logit                 | array of log probabilities | array of logits    \n",
    "| Label (each data point) | float (0.0 or 1.0)    | float (0.0 or 1.0)    | long (class index)         | long (class index) \n",
    "|   Model's last layer    | Sigmoid               | -                     | LogSoftmax                 | -                  \n",
    "|    `weight` argument    | **not** class weights | **not** class weights | class weights              | class weights      \n",
    "|  `pos_weight` argument  | n/a                   | \"weighted\" loss       | n/a                        | n/a                \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "330c949d"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we haven't logged or inspected our losses in real-time. Why bother, if it takes only a minute to train the model? This time is different, though: fine-tuning RoBERTa on more than 67,000 data points, even for a single epoch, will take about 15 min or so in Google Colab. So, let's use a convenient tool to see how our loss is doing as training progresses.\n",
    "\n",
    "#### 6.4.4.1 TensorBoard\n",
    "\n",
    "Yes, TensorBoard is that good! So good that we’ll be using a tool from the competing framework, TensorFlow :-) Jokes aside, TensorBoard is a very useful tool, and PyTorch provides classes and methods so that we can integrate it with\n",
    "our model.\n",
    "\n",
    "First, we need to load TensorBoard’s extension for Jupyter, and then we can run TensorBoard using the newly available magic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The magic above tells TensorBoard to look for logs inside the folder specified by the logdir argument: `runs`. So, there must be a runs folder in the same location as the notebook you’re using to train the model.\n",
    "\n",
    "If you want to know more about running TensorBoard inside notebooks, check this official [guide](https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks).\n",
    "\n",
    "It all starts with the creation of a `SummaryWriter`: since we told TensorBoard to look for logs inside the runs folder, it makes sense to actually log to that folder. Moreover, to be able to distinguish between different\n",
    "experiments or models, we should also specify a sub-folder: `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('runs/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about sending the loss values to TensorBoard? We can use the `add_scalars()` method to send multiple scalar values at once; it needs three arguments:\n",
    "- `main_tag`: the parent name of the tags, or the \"group tag,\" if you will\n",
    "- `tag_scalar_dict`: the dictionary containing the key: value pairs for the scalars you want to keep track of (for example, training and validation losses)\n",
    "- `global_step`: step value; that is, the index you’re associating with the values you’re sending in the dictionary; the index of the mini-batch comes to mind in our case, as losses are computed for each mini-batch\n",
    "\n",
    "As training progresses, you can go back to the cell where TensorBoard was loaded, click on its refresh button on the top right, and observe the current loss level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4.4.2 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7b75295",
    "outputId": "beb68d2c-62a9-4523-8e05-b6aeb41ee3e9"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "batch_losses = []\n",
    "\n",
    "## Training\n",
    "for i, (batch_features, batch_targets) in tqdm(enumerate(datapipes['train'])):\n",
    "    # Set the model's mode\n",
    "    # write your code here\n",
    "    ...\n",
    "    \n",
    "    # Send batch features and targets to the device\n",
    "    # write your code here\n",
    "    ...\n",
    "    \n",
    "    # Step 1 - forward pass\n",
    "    predictions = ...\n",
    "\n",
    "    # Step 2 - computing the loss\n",
    "    loss = ...\n",
    "\n",
    "    # Step 3 - computing the gradients\n",
    "    # Tip: it requires a single method call to backpropagate gradients\n",
    "    # write your code here\n",
    "    ...\n",
    "\n",
    "    batch_losses.append(loss.item())\n",
    "    \n",
    "    writer.add_scalars(main_tag='loss',\n",
    "                       tag_scalar_dict={'training': loss.item()},\n",
    "                       global_step=i)    \n",
    "\n",
    "    # Step 4 - updating parameters and zeroing gradients\n",
    "    # Tip: it takes two calls to optimizer's methods\n",
    "    # write your code here\n",
    "    ...\n",
    "\n",
    "\n",
    "writer.close()\n",
    "    \n",
    "## Validation   \n",
    "with torch.inference_mode():\n",
    "    val_losses = []\n",
    "\n",
    "    for i, (val_features, val_targets) in enumerate(dataloaders['val']):\n",
    "        # Set the model's mode\n",
    "        # write your code here\n",
    "        ...\n",
    "\n",
    "        # Send batch features and targets to the device\n",
    "        # write your code here\n",
    "        ...\n",
    "\n",
    "        # Step 1 - forward pass\n",
    "        predictions = ...\n",
    "\n",
    "        # Step 2 - computing the loss\n",
    "        loss = ...\n",
    "        \n",
    "        val_losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the end of it, your losses on TensorBoard should look more or less like this (if you drag the slider on the right to the maximum level of smoothing):\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch6/tensorboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "724660c3"
   },
   "source": [
    "### 6.4.5 Inference\n",
    "\n",
    "Write a function that takes some text (a sequence of words), a model, its prescribed transformations, and a list of target categories for the classification, and returns the most likely category and the corresponding probability.\n",
    "\n",
    "Since you're handling a single sequence, there's no need for any padding, but you still need to provide a tensor containing a mini-batch (of one) as input to the model.\n",
    "\n",
    "The model returns two logits, one for each class, so you must use the softmax function to convert them into probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a9a8d274"
   },
   "outputs": [],
   "source": [
    "def predict(sequence, model, transforms_fn, categories):        \n",
    "    # Build a tensor of token ids out of the input sequence\n",
    "    # write your code here\n",
    "    ...\n",
    "\n",
    "    # Set the model to the appropriate mode\n",
    "    # write your code here\n",
    "    ...\n",
    "\n",
    "    device = next(iter(model.parameters())).device\n",
    "    \n",
    "    # Use the model to make predictions/logits\n",
    "    # Tip: Don't forget to send the input to the same device as the model\n",
    "    # Tip: Don't forget models take mini-batches as inputs, not single data points\n",
    "    pred = ...\n",
    "    \n",
    "    # Compute the probabilities corresponding to the logits\n",
    "    # and return the top value and index\n",
    "    \n",
    "    probabilities = ...\n",
    "    values, indices = ...\n",
    "    \n",
    "    return [{'label': categories[i], 'value': v.item()} for i, v in zip(indices, values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b587f62f"
   },
   "source": [
    "Now, try out your prediction function and fine-tuned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fe87a17e",
    "outputId": "b451dba7-883f-413b-ac19-6e6a63ca6bbf"
   },
   "outputs": [],
   "source": [
    "categories = ['negative', 'positive']\n",
    "text = \"I am really liking this course\"\n",
    "predict(text, model, xlmr_base.transform(), categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7b50f0c7",
    "outputId": "f9b1f8d0-602a-4339-ce21-a2b3e961a3e4"
   },
   "outputs": [],
   "source": [
    "text = \"This course is too complicated!\"\n",
    "predict(text, model, xlmr_base.transform(), categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "578f344d"
   },
   "source": [
    "That's cool, but what if we could perform sentiment analysis out-of-the-box?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
