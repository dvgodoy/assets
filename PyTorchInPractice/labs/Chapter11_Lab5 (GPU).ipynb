{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb5a719",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71accb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/helper_functions.py\n",
    "from helper_functions import get_image_from_url, save_images, show, xml_to_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c5e9f",
   "metadata": {
    "id": "888c5e9f"
   },
   "source": [
    "## 11.4 Lab 5 / Case 5: Fine-Tuning Object Detection Models\n",
    "\n",
    "In this lab, you'll build a dataset, including data augmentation, and fine-tune a custom object detection model by replacing its standard backbone with a different computer vision model. In the end, you'll evaluate the model using metrics from the COCO challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce02517",
   "metadata": {
    "id": "3ce02517"
   },
   "source": [
    "### 11.4.1 Oxford-IIIT Pet Dataset\n",
    "\n",
    "You'll build a dataset using the images and annotations from the [Oxford-IIIT Pet dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/):\n",
    "\n",
    "\"_We have created a 37 category pet dataset with roughly 200 images for each class. The images have a large variations in scale, pose and lighting. All images have an associated ground truth annotation of breed, head ROI, and pixel level trimap segmentation._\"\n",
    "\n",
    "You will load the data using [PyTorch's built-in class](https://pytorch.org/vision/stable/generated/torchvision.datasets.OxfordIIITPet.html), but you're tasked with preprocessing the annotations and building a dataset that is compatible with V2 transforms for data augmentation (without wrapping the built-in dataset, that is).\n",
    "\n",
    "First, load the data to the a folder of your choice (e.g. `./pets`), making sure to retrieve the `trainval` split (which has annotations), and choose both target types, `category` and `segmentation`, since you'll be fine-tuning a model to detect pets on images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432568a3",
   "metadata": {
    "id": "432568a3"
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import OxfordIIITPet\n",
    "\n",
    "root_folder = './pets'\n",
    "# write the arguments to create an instance of the dataset\n",
    "pets = OxfordIIITPet(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a68cf78",
   "metadata": {
    "id": "2a68cf78"
   },
   "source": [
    "### 11.4.2 Annotations\n",
    "\n",
    "The annotations follow the Pascal VOC challenge format, and are stored as individual XML files, one for each annotated image, inside the `oxford-iiit-pet/annotations/xmls` subfolder. Use the `xml_to_csv()` helper function to convert all these files into a Pandas dataframe and inspect its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfbc4e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "fdfbc4e2",
    "outputId": "528bd747-3f4c-422d-a7ef-3c9450f8dc32"
   },
   "outputs": [],
   "source": [
    "xml_df = ...\n",
    "xml_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0ee5e7",
   "metadata": {
    "id": "eb0ee5e7"
   },
   "source": [
    "The annotations contain the box coordinates in the Pascal VOC system (`[xmin, ymin, xmax, ymax]`), but they only have two main classes, cats and dogs, instead of the expected 37 classes found in the description. As it turns out, there are more files in the `annotations` folder, namely, `list.txt`, `trainval.txt`, and `test.txt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162fd285",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "162fd285",
    "outputId": "003928df-9e5d-4a91-faaa-9ef54d6a5773"
   },
   "outputs": [],
   "source": [
    "# if you chose a different root folder, change it accordingly\n",
    "!ls -l ./pets/oxford-iiit-pet/annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea04722",
   "metadata": {
    "id": "aea04722"
   },
   "source": [
    "Let's take a look at the `list.txt` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befa668e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "befa668e",
    "outputId": "f3d95633-3d81-49f9-fb18-5485dfc04bfd"
   },
   "outputs": [],
   "source": [
    "!head ./pets/oxford-iiit-pet/annotations/list.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48445f85",
   "metadata": {
    "id": "48445f85"
   },
   "source": [
    "It contains a list of all images in the dataset, organized in four columns separated by spaces: Image, CLASS-ID, SPECIES, BREED ID. As it turns out, the \"class\" from the XML file is actually the species. We're interested in the true class ids, from 1 to 37, as stated in the description.\n",
    "\n",
    "Now, let's take a look at the file corresponding to the data you loaded, the `trainval` split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12022f87",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12022f87",
    "outputId": "676e3949-220b-49e8-a338-68817938d9b0"
   },
   "outputs": [],
   "source": [
    "!head ./pets/oxford-iiit-pet/annotations/trainval.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc69f55",
   "metadata": {
    "id": "8fc69f55"
   },
   "source": [
    "It clearly follows the same structure as the previous file, but it does not contain any headers, and it lists only the images that belong to the original train and validation split.\n",
    "\n",
    "We can load it in Pandas for easier visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a85a4a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "c7a85a4a",
    "outputId": "c4824eb0-4198-44ed-a163-864c69b36650"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "trainval_df = pd.read_csv('./pets/oxford-iiit-pet/annotations/trainval.txt', sep=' ', header=None, names=['filename', 'class_id', 'species', 'breed_id'])\n",
    "trainval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b24f47",
   "metadata": {
    "id": "b0b24f47"
   },
   "source": [
    "Each filename has its own corresponding class index (`class_id`), but the label itself, as the descriptive name corresponding to the category is only available as part of the filename itself. We can easily extract it, though:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fade5fcd",
   "metadata": {
    "id": "fade5fcd"
   },
   "outputs": [],
   "source": [
    "trainval_df['category'] = trainval_df['filename'].apply(lambda v: ' '.join([w.capitalize()\n",
    "                                                                            for w in v.split('_')[:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb328f7a",
   "metadata": {
    "id": "cb328f7a"
   },
   "source": [
    "Moreover, there are 3,680 rows, one for each image, but there are 3,687 annotations retrieved from the XML files. Why? It is important to highlight that:\n",
    "- some images may have more than one annotation/box - you saw that already in the Penn-Fudan dataset\n",
    "- some images probably have no annotations/boxes (you'll see that soon)\n",
    "\n",
    "You may use the custom dataset class `ObjDetectionDataset` once again, since it is prepared to take a CSV file or Pandas dataframe containing the annotations (filename, labels, xmin, ymin, xmax, and ymax columns), but keep in mind that only the filenames in the file/dataframe are going to be considered by it.\n",
    "\n",
    "So, if you choose to use the same class, either:\n",
    "- your file/dataframe must also include the filenames that have no annotations\n",
    "- images without annotations won't be included\n",
    "\n",
    "It is better to keep images without annotations as negative cases, so merge both dataframes and make sure that:\n",
    "- every filename is kept, so there are still 3,680 unique filenames after merging\n",
    "- the resulting dataframe has, at least, the following columns: `filename`, `label`, `category`, `xmin`, `ymin`, `xmax`, and `ymax`\n",
    "\n",
    "Besides, use the resulting dataframe to build a `id2label` dictionary to map class id into the corresponding category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064de5ad",
   "metadata": {
    "id": "064de5ad"
   },
   "outputs": [],
   "source": [
    "trainval_df['filename'] = trainval_df['filename'].apply(lambda v: f'{v}.jpg')\n",
    "annotations_df = trainval_df.merge(xml_df, how='left', on='filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b0218f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "52b0218f",
    "outputId": "677aaa7e-af8b-4bfb-ebda-4a24b18ac751"
   },
   "outputs": [],
   "source": [
    "colnames = ['filename', 'label', 'category', 'width', 'height', 'xmin', 'ymin', 'xmax', 'ymax']\n",
    "annotations_df = annotations_df.rename(columns={'class_id': 'label'})[colnames]\n",
    "annotations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1710e5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "db1710e5",
    "outputId": "a410f7ba-8a61-44c1-de74-c0d57efcf917"
   },
   "outputs": [],
   "source": [
    "id2label = dict(annotations_df[['label', 'category']].drop_duplicates().values)\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7d39d2",
   "metadata": {
    "id": "fa7d39d2"
   },
   "outputs": [],
   "source": [
    "assert len(annotations_df['filename'].unique()) == 3680\n",
    "assert len(id2label.values()) == 37\n",
    "assert len(annotations_df) == 3681"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2e25b4",
   "metadata": {
    "id": "2f2e25b4"
   },
   "source": [
    "Shouldn'it be 3,687? Perhaps even more, since it should also include images without any annotations? It actually should, but some of the annotated images were excluded from the `trainval.txt` list of files for some unknown reason. In case you're curious, these are the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e528296e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e528296e",
    "outputId": "89dbbc3a-1546-47c9-e176-bf79c216fb8c"
   },
   "outputs": [],
   "source": [
    "extra_annotations = set(xml_df['filename'].unique()).difference(set(annotations_df['filename'].unique()))\n",
    "extra_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d19d78",
   "metadata": {
    "id": "60d19d78"
   },
   "source": [
    "The whole point of this apparent detour from our main job here - fine-tuning an object detection model - is to illustrate the fact that every dataset has its issues, and you should always take your time to investigate how it's organized, if there are quality issues, and ensure it's in the right shape to be loaded into an instance of your dataset class.\n",
    "\n",
    "By the way, PyTorch's built-in dataset class for the Oxford-IIIT Pet Dataset handles this preprocssing (splitting filenames, building id2label dictionary, etc) in its [constructor method](https://pytorch.org/vision/main/_modules/torchvision/datasets/oxford_iiit_pet.html), in case you'd like to check it out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6986c8",
   "metadata": {
    "id": "9e6986c8"
   },
   "source": [
    "### 11.4.3 Train-Validation Split\n",
    "\n",
    "The original list of files does not give any indication regarding the split between training and validation sets, so you'll have to do it yourself.\n",
    "\n",
    "Our suggestion is to shuffle the filenames, and take a large part of them (e.g. 3,000) as training set, and the remaining files as validation set.\n",
    "\n",
    "Split the annotations dataframe in two, as the filenames in each dataframe determine which files are going be part of each dataset (assuming you're using our `ObjDetectionDataset`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a20f284",
   "metadata": {
    "id": "3a20f284"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(11)\n",
    "\n",
    "# Get all (unique) file names from the annotations dataframe\n",
    "fnames = ...\n",
    "np.random.shuffle(fnames)\n",
    "\n",
    "# Create a boolean pandas series to determine if a given annotation belongs\n",
    "# to the training set\n",
    "# Tip: don't forget that images may have multiple annotations - make sure\n",
    "# two annotations of the same image don't end up in different sets\n",
    "is_train = ...\n",
    "\n",
    "annotations = {}\n",
    "# Use the boolean series to slice the annotations dataframe\n",
    "annotations['train'] = ...\n",
    "annotations['val'] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5082d0b1",
   "metadata": {
    "id": "5082d0b1"
   },
   "source": [
    "### 11.4.4 Loading Model's Weights\n",
    "\n",
    "You're using a new backbone for your Faster R-CNN model, so you need to pick one that's different from ResNet50. You could, for example, choose a smaller model from the ResNet family, but it's likely more fun to choose a completely different model instead. We suggest you use MobileNet V2 as the new backbone.\n",
    "\n",
    "Once you choose the model, load its pretrained weights and the prescribed transformations that come with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2Botj0epelN4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Botj0epelN4",
    "outputId": "ab8dcd5f-43fe-4edc-d0f4-7670aae5a6dd"
   },
   "outputs": [],
   "source": [
    "from torchvision.models import get_weight\n",
    "\n",
    "weights = ...\n",
    "transforms_fn = ...\n",
    "transforms_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15dda4e",
   "metadata": {
    "id": "c15dda4e"
   },
   "source": [
    "This is its `forward()` method (of MobileNet V2 transform, that is). Take a good look at the sequence of transformations it performs because, as you probably already guesses, this function is not compatible with V2 transforms, so you'll have to include them yourself - if needed - in your data augmentation pipeline (the next section).\n",
    "\n",
    "```python\n",
    "def forward(self, img: Tensor) -> Tensor:\n",
    "    img = F.resize(img, self.resize_size, interpolation=self.interpolation, antialias=self.antialias)\n",
    "    img = F.center_crop(img, self.crop_size)\n",
    "    if not isinstance(img, Tensor):\n",
    "        img = F.pil_to_tensor(img)\n",
    "    img = F.convert_image_dtype(img, torch.float)\n",
    "    img = F.normalize(img, mean=self.mean, std=self.std)\n",
    "    return img\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425ab7a6",
   "metadata": {
    "id": "425ab7a6"
   },
   "source": [
    "### 11.4.5 Data Augmentation\n",
    "\n",
    "It is time to write your own `get_transform()` function that takes one argument, namely, ìf it is performing transformations on the training or the validation set:\n",
    "- if it is the validation set, it should stick to the basics (hint: check the prescribed transformations to assess these points)\n",
    "  - make sure the image is in the right size/shape for the backbone of your choice\n",
    "  - convert, if needed, PIL images to tensors\n",
    "  - normalize the values\n",
    "- if it is in the training set, it may perform data augmentation as well:\n",
    "  - choose one or more data augmenting transformations\n",
    "  - sanitize bounding boxes, just in case\n",
    "\n",
    "Pay special attention to the order in which transformations will happen, to make sure the transformed image at the end of the pipeline does indeed match the requirements of the backbone model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27ec8ca",
   "metadata": {
    "id": "b27ec8ca"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import defaultdict\n",
    "from torchvision.transforms import v2 as transforms\n",
    "\n",
    "augmenting = [\n",
    "    # Choose one (or more) augmentation transform(s), such as RandomHorizontalFlip, for example\n",
    "    # write your code here\n",
    "    ...\n",
    "]\n",
    "\n",
    "basic = [\n",
    "    # Include required transformations here, such as transforming PIL images into tensors\n",
    "    # and normalizing pixel values\n",
    "    # write your code here\n",
    "    ...\n",
    "]\n",
    "\n",
    "def get_transform(train):\n",
    "    ops = [\n",
    "        # Include resizing transformations here, to make images the right size for the chosen model\n",
    "        # write your code here\n",
    "        ...\n",
    "    ]\n",
    "    # Only does augmenting in training mode\n",
    "    if train:\n",
    "        ops.extend(augmenting)\n",
    "    # Basic transforms: to tensor, sanitizing, and normalizing\n",
    "    ops.extend(basic)\n",
    "    return transforms.Compose(ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bfd44b",
   "metadata": {
    "id": "b5bfd44b"
   },
   "source": [
    "### 11.4.6 Datasets and DataLoaders\n",
    "\n",
    "Create two datasets, one for training, and one for validation, and assign the corresponding transformations to each one of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e65871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torchvision.datapoints import Image, BoundingBox, BoundingBoxFormat, Mask\n",
    "from torchvision.ops import masks_to_boxes, box_area\n",
    "from torchvision.datasets import VisionDataset\n",
    "\n",
    "class ObjDetectionDataset(VisionDataset):\n",
    "    def __init__(self, image_folder, annotations=None, mask_folder=None, transforms=None):\n",
    "        super().__init__(image_folder, transforms, None, None)\n",
    "        self.image_folder = image_folder\n",
    "        self.annotations = annotations\n",
    "        self.mask_folder = mask_folder\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.images = list(sorted(os.listdir(image_folder)))\n",
    "\n",
    "        self.df_boxes = None\n",
    "        assert (annotations is not None) or (mask_folder is not None), \"At least one, annotations or masks, must be supplied\"\n",
    "        if annotations is not None:\n",
    "            if isinstance(annotations, str):\n",
    "                self.df_boxes = pd.read_csv(annotations)\n",
    "            else:\n",
    "                self.df_boxes = annotations\n",
    "            assert len(set(self.df_boxes.columns).intersection({'filename', 'xmin', 'ymin', 'xmax', 'ymax'})) == 5, \"Missing columns in CSV\"\n",
    "            self.images = self.df_boxes['filename'].unique().tolist()\n",
    "\n",
    "        self.masks = None\n",
    "        if mask_folder is not None:\n",
    "            self.masks = list(sorted(os.listdir(mask_folder)))\n",
    "            assert len(self.masks) == len(self.images), \"Every image must have one, and only one, mask\"\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filename = os.path.join(self.image_folder, self.images[idx])\n",
    "        image_tensor = read_image(image_filename, mode=ImageReadMode.RGB)\n",
    "        image_hw = image_tensor.shape[-2:]\n",
    "\n",
    "        labels = None\n",
    "        # If there are masks, we work with them\n",
    "        if self.masks is not None:\n",
    "            mask_filename = os.path.join(self.mask_folder, self.masks[idx])\n",
    "            merged_mask = read_image(mask_filename)\n",
    "            instances = merged_mask.unique()[1:]\n",
    "\n",
    "            masks = (merged_mask == instances.view(-1, 1, 1))\n",
    "            boxes = masks_to_boxes(masks)\n",
    "\n",
    "            wrapped_masks = Mask(masks)\n",
    "        # No masks, so we fallback to a DF of annotated boxes\n",
    "        else:\n",
    "            annots = self.df_boxes.query(f'filename == \"{self.images[idx]}\"')\n",
    "            boxes = torch.as_tensor(annots.dropna()[['xmin', 'ymin', 'xmax', 'ymax']].values)\n",
    "            if 'label' in annots.columns:\n",
    "                labels = torch.as_tensor(annots.dropna()['label'].values)\n",
    "            wrapped_masks = None\n",
    "\n",
    "        wrapped_boxes = BoundingBox(boxes, format=BoundingBoxFormat.XYXY, spatial_size=image_hw)\n",
    "        num_objs = len(boxes)\n",
    "\n",
    "        if len(boxes):\n",
    "            if labels is None:\n",
    "                # if there are no labels, we assume every instance is of\n",
    "                # the same, and only, class\n",
    "                labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "            area = box_area(wrapped_boxes)\n",
    "        else:\n",
    "            # Only background, no boxes\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "            area = torch.tensor([0.], dtype=torch.float32)\n",
    "\n",
    "        target = {\n",
    "            'boxes': wrapped_boxes,\n",
    "            'area': area,\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([idx+1]),\n",
    "            'iscrowd': torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        }\n",
    "        if wrapped_masks is not None:\n",
    "            target['masks'] = wrapped_masks\n",
    "\n",
    "        image = Image(image_tensor)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image, target = self.transforms(image, target)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c7d7d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "84c7d7d2",
    "outputId": "3747fac2-e727-475c-f530-89cd62e4c89a"
   },
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "datasets['train'] = ...\n",
    "datasets['val'] = ...\n",
    "\n",
    "len(datasets['train']), len(datasets['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecac31d",
   "metadata": {
    "id": "5ecac31d"
   },
   "source": [
    "Next, create two data loaders, one for each dataset. You should shuffle the training set, but not the validation one. Also, keep batch size small (e.g. two) to avoid out-of-memory issues in the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f19e95",
   "metadata": {
    "id": "c9f19e95"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloaders = {}\n",
    "dataloaders['train'] = ...\n",
    "dataloaders['val'] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2ce997",
   "metadata": {
    "id": "5f2ce997"
   },
   "source": [
    "Try fetching a mini-batch from your training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fd55e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "78fd55e3",
    "outputId": "eb346acc-f168-4763-cfd6-495061744f82"
   },
   "outputs": [],
   "source": [
    "next(iter(dataloaders['train']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed526b2b",
   "metadata": {
    "id": "ed526b2b"
   },
   "source": [
    "Did you get an error? No? Consider yourself lucky! At some point, it will raise an error, whenever an image with either zero or more than one annotation is included in the mini-batch.\n",
    "\n",
    "The collate function is the function used by the data loader to patch together multiple data points into a mini-batch. If your dataset is nothing but tensors, that's trivial: it only has to stack them up. Stacking them up, though, assumes every data point has exactly the same shape for its features.\n",
    "\n",
    "In object detection models, though, this is not guaranteed to be the case: one image may have no boxes, another one may have three boxes, and yet another one may have only one. Those cannot be stacked together.\n",
    "\n",
    "The solution, fortunately, is pretty easy, and it looks like this:\n",
    "\n",
    "```python\n",
    "lambda batch: tuple(zip(*batch))\n",
    "```\n",
    "\n",
    "Throw the lambda function above as the `collate_fn` argument of your data loaders, and try again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaa06ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8aaa06ba",
    "outputId": "6989dc53-710d-4f8d-8a17-c0ce7e23a1e8"
   },
   "outputs": [],
   "source": [
    "dataloaders = {}\n",
    "dataloaders['train'] = ...\n",
    "dataloaders['val'] = ...\n",
    "\n",
    "next(iter(dataloaders['train']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71690ee",
   "metadata": {
    "id": "b71690ee"
   },
   "source": [
    "### 11.4.7 Model\n",
    "\n",
    "Now, the fun part begins: replacing the backbone of a pretrained object detection model!\n",
    "\n",
    "You will have to create a brand new instance of the `FasterRCNN` class using the required arguments to make your model work:\n",
    "   - `backbone`: your feature extractor\n",
    "   - `rpn_anchor_generator`: the new anchor generator\n",
    "   - `box_roi_pool`: the new ROI pooler\n",
    "   - `num_classes`: the number of classes for your task\n",
    "\n",
    "You already know the number of classes - but don't forget another one for the negative case, that is, whenever there's no object in the image. This class (for the background, if you will) is usually assigned the zero index (and that's why the class indices from the dataset start at one).\n",
    "\n",
    "You also have the weights for the backbone model too, but you need to create a model that returns its features only (the \"headless\" model, as seen in Chapter 2). The model must return either a feature map dictionary (if you're extracing features from multiple layers of your backbone) or a single tensor (if you're extracting a single set of features). Also, keep in mind that:\n",
    "   - some models (like [MobileNet V2](https://pytorch.org/hub/pytorch_vision_mobilenet_v2/), our suggested choice of new bacbone) can have its features extracted easily accesing a single attribute (`features` in the case of MobileNet)\n",
    "   - for more complex models, you can use `create_feature_extractor()` or `IntermediateLayerGetter` to build your backbone\n",
    "\n",
    "Use the weights you already loaded to create an instance of your backbone model and use one of the alternatives above to get its features only returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a6e509",
   "metadata": {
    "id": "01a6e509"
   },
   "outputs": [],
   "source": [
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models import mobilenet_v2\n",
    "\n",
    "num_classes = len(id2label) + 1\n",
    "\n",
    "weights = ...\n",
    "mobilenet = ...\n",
    "new_backbone = mobilenet.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f9dbd7",
   "metadata": {
    "id": "51f9dbd7"
   },
   "source": [
    "Double-check if your model is returning what you expect of it by feeding it a random tensor in the shape of a mini-batch (make sure the height and width of your random images match the expected input of your model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634835fd",
   "metadata": {
    "id": "634835fd"
   },
   "outputs": [],
   "source": [
    "dummy_x = torch.randn(2, 3, 224, 224)\n",
    "dummy_output = new_backbone(dummy_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ed7437",
   "metadata": {
    "id": "81ed7437"
   },
   "source": [
    "You shouldn't get any errors, and your dummy output must be either a single tensor, or a feature map dictionary. Check the shape of each returned tensor (one or more), and make sure they all have the same number of output channels. This is required by the Faster R-CNN architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b91bce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71b91bce",
    "outputId": "2e799c93-831a-476d-d0ed-1fc5c6697490"
   },
   "outputs": [],
   "source": [
    "out_channels = dummy_output.shape[1]\n",
    "out_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7622af",
   "metadata": {
    "id": "fd7622af"
   },
   "source": [
    "Assign the number of output channels to the instance of your backbone as an `out_channels` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eb119b",
   "metadata": {
    "id": "77eb119b"
   },
   "outputs": [],
   "source": [
    "new_backbone.out_channels = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b384ea03",
   "metadata": {
    "id": "b384ea03"
   },
   "source": [
    "Create an instance of the `AnchorGenerator` class, and make sure each argument - `sizes` and `aspect_ratios` is a tuple containing as many elements as the number of feature maps returned by your backbone.\n",
    "\n",
    "Each element is a tuple itself, and may have as many elements as you wish. For more details, refer to the \"Region Proposal Network\" subsection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20171de",
   "metadata": {
    "id": "a20171de"
   },
   "outputs": [],
   "source": [
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "sizes = ((32, 64, 128, 256, 512),)\n",
    "aspect_ratios = ((0.5, 1.0, 2.0),)\n",
    "\n",
    "anchor_generator = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33b0591",
   "metadata": {
    "id": "c33b0591"
   },
   "source": [
    "Create an instance of the `MultiScaleRoIAlign` class, and make sure it points to at least one valid feature map as returned by our backbone model. For more details, refer to the \"Regions of Interest\" subsection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c932be4",
   "metadata": {
    "id": "7c932be4"
   },
   "outputs": [],
   "source": [
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "\n",
    "output_size = 7\n",
    "sampling_ratio = 2\n",
    "\n",
    "# Tip: simpler models don't return dictionaries, but feature maps are guaranteed to be a dictionary\n",
    "# containing, at least, a \"0\" key\n",
    "roi_pooler = MultiScaleRoIAlign(featmap_names=['0'], output_size=7, sampling_ratio=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf1004c",
   "metadata": {
    "id": "9bf1004c"
   },
   "source": [
    "Now, put everything together as your own Faster R-CNN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76fa345",
   "metadata": {
    "id": "e76fa345"
   },
   "outputs": [],
   "source": [
    "model = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4e0d86",
   "metadata": {
    "id": "3e4e0d86"
   },
   "source": [
    "There you go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8387e1",
   "metadata": {
    "id": "5f8387e1"
   },
   "source": [
    "#### 11.4.7.1 Double-Checking the Model\n",
    "\n",
    "To make sure your configuration is working fine, you can feed your new Faster R-CNN model a random tensor representing a dummy mini-batch once again. If you don't get any errors back, you're likely good to go!\n",
    "\n",
    "Don't forget to send each tensor in your mini-batch, individually, to the device. You cannot simply send them all at once as you used to do before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7b8072",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8b7b8072",
    "outputId": "e26ef5d7-a223-4fcf-8257-6430c3c2a262"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "images, targets = next(iter(dataloaders['train']))\n",
    "\n",
    "# Send images and targets to device\n",
    "# write your code here\n",
    "...\n",
    "\n",
    "# Make predictions using your model - you should get a dict of losses back\n",
    "output = ...\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56442045",
   "metadata": {
    "id": "56442045"
   },
   "source": [
    "#### 11.4.7.2 Recap\n",
    "\n",
    "In order to replace the backbone of a Faster R-CNN model, you need to:\n",
    "\n",
    "1. Choose a computer vision model (e.g. ResNet, MobileNet)\n",
    "2. Create a backbone by extracting the model features as a feature map (dict) or a single tensor:\n",
    "   - if there are multiple feature maps, make sure they all produce the same number of output channels\n",
    "   - some models (like MobileNet) can have its features extracted easily accesing a single attribute\n",
    "   - for more complex models, you can use `create_feature_extractor()` or `IntermediateLayerGetter` to build your backbone\n",
    "   - create and/or set the `out_channels` attribute of your backbone\n",
    "3. Create and configure an `AnchorGenerator` so it has as many `sizes` and `aspect_ratios` as feature maps returned by your backbone\n",
    "4. Create and configure a ROI pooler `MultiScaleRoIAlign` so it points to the correct feature maps returned by your backbone\n",
    "5. Create an instance of `FasterRCNN` with the proper arguments:\n",
    "   - `backbone`: your feature extractor\n",
    "   - `rpn_anchor_generator`: the new anchor generator\n",
    "   - `box_roi_pool`: the new ROI pooler\n",
    "   - `num_classes`: the number of classes for your task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63162919",
   "metadata": {
    "id": "63162919"
   },
   "source": [
    "### 11.4.8 Learning Rate Schedulers\n",
    "\n",
    "We need to talk about learning rates. So far, we've always used an optimizer with a fixed learning rate. The Adam optimizer actually makes adjustments under the hood but, as far as we're concerned, the learning rate was defined as a single value for the whole training loop.\n",
    "\n",
    "Unfortunately, this straightforward approach does not always work. In some cases, and training a Faster R-CNN model is one of those cases, it may be necessary to gradually \"warm-up\" the learning rate up to the desired level. Conversely, sometimes you may need to actually decrease the learning rate after a few epochs or updates. Or, perhaps, combine both approaches: warm-up at start, run it at certain level for a while, and then start decreasing it.\n",
    "\n",
    "Luckily, it's not hard to accomplish this at all: there's a learning rate scheduler for every need. Let's say you’d like to reduce the learning rate by one order of magnitude (that is, multiplying it by 0.1) every T epochs, such that training is faster at the beginning and slows down after a while to try avoiding convergence problems. Set up a scheduler to do that.\n",
    "\n",
    "Now, let's say you'd like to warm-up the learning rate from zero all the way up to a predefined level over the course of the first epoch (you'll actually do that). Set up another scheduler to do that too.\n",
    "\n",
    "The learning rate scheduer does modify the underlying learning rate set in the optimizer, so it should be no surprise that one of the scheduler's arguments is the optimizer itself. The learning rate set for the optimizer will be the initial learning rate of the scheduler. The scheduler also has a `step()` method, just like the optimizer does, and you should call the scheduler's `step()` method after calling the optimizer's.\n",
    "\n",
    "The schedulers may be divided into two main groups, according to the where they are expected to be called during the training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ceaf38",
   "metadata": {
    "id": "68ceaf38"
   },
   "source": [
    "#### 11.4.8.1 Epoch Schedulers\n",
    "\n",
    "These schedulers will have their `step()` method called at the end of every epoch. But each scheduler has its own rules for updating the learning rate. Here are a few examples from [PyTorch's learning rate schedulers](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate):\n",
    "\n",
    "- `StepLR`: it multiplies the learning rate by a factor `gamma` every `step_size` epochs.\n",
    "- `LinearLR`: it starts with a learning rate that's a fraction (`start_factor`) of the optimizer's learning rate, and modifies it linearly over `total_iters` updates until it reaches the end learning rate (`end_factor`, which defaults to 1.0, times the optimizer's learning rate)\n",
    "- `MultiStepLR`: it multiplies the learning rate by a factor `gamma` at the epochs indicated in the list of `milestones`.\n",
    "- `ExponentialLR`: it multiplies the learning rate by a factor `gamma` every epoch, no exceptions.\n",
    "- `LambdaLR`: it takes your own customized function that should take the epoch as an argument and returns the corresponding multiplicative factor (with respect to the initial learning rate).\n",
    "\n",
    "#### 11.4.8.2 Validation Loss Scheduler\n",
    "\n",
    "`ReduceLROnPlateau` scheduler should also have its `step()` method called at the end of every epoch, but it has its own group here because it does not follow a predefined schedule. Ironic, right?\n",
    "\n",
    "The `step()` method takes the validation loss as an argument, and the scheduler can be configured to tolerate a lack of improvement in the loss (to a threshold, of course) up to a given number of epochs (the aptly named `patience` argument). After the scheduler runs out of patience, it updates the learning rate, multiplying it by the `factor` argument (for the schedulers listed in the last section, this factor was named `gamma`).\n",
    "\n",
    "#### 11.4.8.3 Mini-Batch Schedulers\n",
    "\n",
    "Some schedulers should have their `step()` method called at the end of every mini-batch, like cyclical schedulers:\n",
    "\n",
    "- `CyclicLR`: This cycles between `base_lr` and `max_lr` (so it disregards the initial learning rate set in the optimizer), using step_size_up updates to go from the base to the max learning rate, and step_size_down updates to go back. This behavior corresponds to mode=triangular. Additionally, it is possible to shrink the amplitude using different modes: triangular2 will halve the amplitude after each cycle, while exp_range will exponentially shrink the amplitude using gamma as base and the number of the cycle as the exponent.\n",
    "- `OneCycleLR`: This uses a method called annealing to update the learning rate from its initial value up to a defined maximum learning rate (`max_lr`) and then down to a much lower learning rate over a `total_steps` number of updates, thus performing a single cycle.\n",
    "\n",
    "The fact that the `LinearLR` scheduler belongs to the \"Epoch Schedulers\" group doesn't mean you're not allowed to use it as a mini-batch scheduler. You're free to use update the learning rate at your convenience and you'll be doing exactly that but, first, let's take a couple of scheduler for a spin!\n",
    "\n",
    "Start by creating an optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba30334",
   "metadata": {
    "id": "7ba30334"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR, LinearLR\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8dd6c8",
   "metadata": {
    "id": "ac8dd6c8"
   },
   "source": [
    "You'll want to track how its learning rate is modified over the course of the mini-batches and epochs, so the helper function below is, well, helping you with that by unpacking the internal learning rate of the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7039d6ea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7039d6ea",
    "outputId": "28ea8f09-8ea5-4186-b78b-a7fb604dfc8d"
   },
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    return list(map(lambda d: d['lr'], optimizer.state_dict()['param_groups']))\n",
    "\n",
    "get_lr(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1929251f",
   "metadata": {
    "id": "1929251f"
   },
   "source": [
    "You'll have to create two schedulers now:\n",
    "- one that reduces the learning rate by a ten-fold factor after three epochs\n",
    "- another one that increases the learning rate from zero to the value set in the optimizer over the course of one epoch (hint: it has to update the learning rate as many times as there are mini-batches in your training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b5b204",
   "metadata": {
    "id": "c0b5b204"
   },
   "outputs": [],
   "source": [
    "lr_scheduler = ...\n",
    "\n",
    "warmup_factor = 1.0 / 1000\n",
    "warmup_iters = min(1000, len(dataloaders['train']) - 1)\n",
    "lr_scheduler2 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f62a85",
   "metadata": {
    "id": "e3f62a85"
   },
   "source": [
    "Now, place each scheduler's `step()` method in the dummy training loop below so they behave as described above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d074f18",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5d074f18",
    "outputId": "3ca9a0aa-86fb-4dfa-d89b-5a7f3f779225"
   },
   "outputs": [],
   "source": [
    "# Recreating everything here, so you don't have to re-run the previous code\n",
    "# if you want to try different configurations or places in the loop\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = ...\n",
    "lr_scheduler2 = ...\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "lrs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(len(dataloaders['train'])):\n",
    "        # write your code here?\n",
    "        ...\n",
    "\n",
    "        lrs.append(get_lr(optimizer)[0])\n",
    "\n",
    "    # write your code here?\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df34fa0",
   "metadata": {
    "id": "3df34fa0"
   },
   "source": [
    "You should get a \"curve\" like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d6e295",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "id": "88d6e295",
    "outputId": "4ecfc3cc-0034-4509-a0bc-d043a78a56f4"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(lrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450b9b34",
   "metadata": {
    "id": "450b9b34"
   },
   "source": [
    "### 11.4.9 Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34ddcdd",
   "metadata": {
    "id": "e34ddcdd"
   },
   "source": [
    "It is time to write a real training loop now! You can use the dummy loop as a template and build on top of it, once you're happy with your schedulers.\n",
    "\n",
    "Don't forget to send every tensor, individually, to the same device as the model. Also, keep in mind that the model returns a dictionary with many separate losses. It is your job to sum them all up to compute gradients based on the total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da8ccad",
   "metadata": {
    "id": "7da8ccad"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Decreases the learning rate by 10x every 3 epochs\n",
    "lr_scheduler = ...\n",
    "\n",
    "# Warms-up the learning rate from zero to 0.005 over one epoch\n",
    "warmup_factor = 1.0 / 1000\n",
    "warmup_iters = min(1000, len(dataloaders['train']) - 1)\n",
    "\n",
    "lr_scheduler2 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990a9910",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "990a9910",
    "outputId": "202095d9-776a-4a83-e220-c33f6f0b6e09"
   },
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, targets) in enumerate(dataloaders['train']):\n",
    "        # Send images and targets to device\n",
    "        # write your code here\n",
    "        images = ...\n",
    "        targets = ...\n",
    "\n",
    "        # Set your model's mode\n",
    "        # write your code here\n",
    "        ...\n",
    "\n",
    "        # Call the model to get a loss dict back\n",
    "        loss_dict = ...\n",
    "        \n",
    "        if not (i % 50):\n",
    "            print([(k, v.item()) for k, v in loss_dict.items()])\n",
    "\n",
    "        # You have many losses in the dict, but you can only\n",
    "        # call backward one a single value, so you must\n",
    "        # add them up\n",
    "        losses = ...\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Make a call to a scheduler - maybe here?\n",
    "        # write your code here\n",
    "        ...\n",
    "\n",
    "    # Make a call to a scheduler - maybe here?\n",
    "    # write your code here\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28134429",
   "metadata": {
    "id": "28134429"
   },
   "source": [
    "Training this model takes quite a while..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81468794",
   "metadata": {
    "id": "81468794"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'mobilenet_v2_pets.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e12689e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7e12689e",
    "outputId": "317ba338-7457-46f7-c18d-27eeb9b09ff1"
   },
   "outputs": [],
   "source": [
    "# # mobilenet_v2_pets.pth\n",
    "# !wget https://github.com/dvgodoy/assets/releases/download/model/mobilenet_v2_pets.pth\n",
    "# state = torch.load('./mobilenet_v2_pets.pth', map_location='cpu')\n",
    "# model.load_state_dict(state)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdda7d2",
   "metadata": {
    "id": "4cdda7d2"
   },
   "source": [
    "### 11.4.10 Trying It Out\n",
    "\n",
    "Let's take an image from our validation set and see if our model can correctly detect an object , cat or dog, in the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0d84ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6c0d84ba",
    "outputId": "8bccd089-6ed5-4807-e732-5c609dd9f81f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 550\n",
    "x, y = datasets['val'][i]\n",
    "model.eval()\n",
    "images = list(x.unsqueeze(0).to(device))\n",
    "targets = [{k: v.to(device) for k, v in y.items()}]\n",
    "\n",
    "pred = model(images, targets)\n",
    "pred, y['boxes']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b2946c",
   "metadata": {
    "id": "a9b2946c"
   },
   "source": [
    "It predicted a box indeed, but is it any good?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bf2bd9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "id": "01bf2bd9",
    "outputId": "b2501ecf-501f-4282-d9de-38c4bc1a0a3a"
   },
   "outputs": [],
   "source": [
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "boxes = pred[0]['boxes'][0].detach().cpu().unsqueeze(0)\n",
    "img = read_image(f\"./pets/oxford-iiit-pet/images/{datasets['val'].images[i]}\")\n",
    "result = draw_bounding_boxes(img, boxes, colors=['red'], width=3)\n",
    "ToPILImage()(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76986c5",
   "metadata": {
    "id": "d76986c5"
   },
   "source": [
    "Not at all! It is awfully misplaced! Does it mean we should throw the model away and start from scratch?\n",
    "\n",
    "Not so fast... let's dig a little bit deeper and try figuring it out first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d72ce9",
   "metadata": {
    "id": "61d72ce9"
   },
   "source": [
    "#### 11.4.10.1 Predicted vs Original Boxes\n",
    "\n",
    "First, we need to rule out an annotation issue. Who knows if the box was correctly drawn in the first place, right?\n",
    "\n",
    "So, let's start by retrieving the annotated bounding box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e29b8cb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3e29b8cb",
    "outputId": "8ab9e13f-3eac-47c8-98e8-2654fc347691"
   },
   "outputs": [],
   "source": [
    "fname = datasets['val'].images[i]\n",
    "orig_box = torch.as_tensor(annotations['val'].query(f'filename == \"{fname}\"').values[0][-4:].astype(np.float32)).unsqueeze(0)\n",
    "orig_box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ffd8fb",
   "metadata": {
    "id": "91ffd8fb"
   },
   "source": [
    "Then, let's plot the two boxes, the annotated one (in green) and the predicted one (in red) next to each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc315df3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "id": "bc315df3",
    "outputId": "e202164d-9360-4ded-fd67-b5dfe553f0cb"
   },
   "outputs": [],
   "source": [
    "boxes = pred[0]['boxes'][0].detach().cpu().unsqueeze(0)\n",
    "img = read_image(f\"./pets/oxford-iiit-pet/images/{datasets['val'].images[i]}\")\n",
    "result = draw_bounding_boxes(img, torch.cat([boxes, orig_box]), colors=['red', 'green'], width=3)\n",
    "ToPILImage()(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8960d9e8",
   "metadata": {
    "id": "8960d9e8"
   },
   "source": [
    "Clearly, the annotation is good, so what else could be the issue here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0048444d",
   "metadata": {
    "id": "0048444d"
   },
   "source": [
    "#### 11.4.10.2 Predicted vs Transformed Boxes\n",
    "\n",
    "Remember that, even if we didn't perform data augmentation on the validation set, it still goes through a few transformations to match the required input shape required by the MobileNet V2 model.\n",
    "\n",
    "So, instead of drawing the (raw) annotated box on top of the original image, let's take both the box and image after these transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e384ce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "c7e384ce",
    "outputId": "19237f11-48a0-48dc-f16d-6de48fa49ce8"
   },
   "outputs": [],
   "source": [
    "boxes = pred[0]['boxes'][0].detach().cpu().unsqueeze(0)\n",
    "img = read_image(f\"./pets/oxford-iiit-pet/images/{datasets['val'].images[i]}\")\n",
    "img = transforms.Compose([transforms.Resize(232, antialias=True), transforms.CenterCrop(224)])(img)\n",
    "result = draw_bounding_boxes(img, torch.cat([boxes, y['boxes']]), colors=['red', 'green'], width=3)\n",
    "ToPILImage()(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7377e291",
   "metadata": {
    "id": "7377e291"
   },
   "source": [
    "Much, much better! The predicted and (transformed) annotated box are very similar!\n",
    "\n",
    "So, what's happening here?\n",
    "\n",
    "The original Faster R-CNN model used a ResNet50 backbone and included both pre- and post-processing routines out-of-the-box. By replacing the backbone with our own, we had to handle the preprocessing transformations ourselves, so it's only logical we do the same for the post-processing too, right? That's the missing piece in our workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ef1191",
   "metadata": {
    "id": "a5ef1191"
   },
   "source": [
    "### 11.4.11 Postprocessing Boxes\n",
    "\n",
    "Most preprocessing transformations for computer vision models include resizing and center cropping, so that's where we're focusing here. We'll resize and shift the boxes according to the estimated resizing and cropping. The `restore_boxes()` helper function does exactly that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2019fc91",
   "metadata": {
    "id": "2019fc91"
   },
   "outputs": [],
   "source": [
    "from torchvision.models.detection.transform import resize_boxes\n",
    "\n",
    "def restore_boxes(orig_image_dims, transf_image_dims, boxes):\n",
    "    is_wide = (orig_image_dims [1] >= orig_image_dims[0])\n",
    "    shortest = min(orig_image_dims)\n",
    "    longest = max(orig_image_dims)\n",
    "    shift = (longest-shortest)/2\n",
    "    if is_wide:\n",
    "        shift = torch.as_tensor([shift, 0, shift, 0])\n",
    "    else:\n",
    "        shift = torch.as_tensor([0, shift, 0, shift])\n",
    "\n",
    "    # if it's a perfect square, we assume it was cropped that size\n",
    "    is_cropped = (transf_image_dims[0] == transf_image_dims[1])\n",
    "    boxes = resize_boxes(boxes, transf_image_dims, (shortest, shortest) if is_cropped else orig_image_dims)\n",
    "\n",
    "    ratio = longest/shortest\n",
    "    is_square = (abs(ratio - 1) < 0.01)\n",
    "    if not is_square:\n",
    "        boxes += shift\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0516ed15",
   "metadata": {
    "id": "0516ed15"
   },
   "source": [
    "What does the (resized) predicted box look like when drawn on top of the original image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f873c66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "id": "7f873c66",
    "outputId": "e215af74-daa4-45dd-fba2-354b5a321675"
   },
   "outputs": [],
   "source": [
    "img = read_image(f\"./pets/oxford-iiit-pet/images/{datasets['val'].images[i]}\")\n",
    "orig_image_dims = img.shape[1:]\n",
    "transf_image_dims = x.shape[1:]\n",
    "boxes = pred[0]['boxes'][0].detach().cpu().unsqueeze(0)\n",
    "boxes = restore_boxes(orig_image_dims, transf_image_dims, boxes)\n",
    "\n",
    "result = draw_bounding_boxes(img, boxes, colors=['red']*boxes.shape[0], width=3)\n",
    "ToPILImage()(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eb6c18",
   "metadata": {
    "id": "06eb6c18"
   },
   "source": [
    "It looks awesome now!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed95e54d",
   "metadata": {},
   "source": [
    "Congratulations! You have successfully gone through the development and training of your first customized Faster R-CNN model, including data cleaning and organizing, and using learning rate schedulers. That's quite an accomplishment!\n",
    "\n",
    "But that's quite subjective, right? It's not a true evaluation. Let's talk about that now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32cf48e",
   "metadata": {
    "id": "b32cf48e"
   },
   "source": [
    "### 11.4.12 Evaluation\n",
    "\n",
    "The evaluation of object detection models is quite involved, so we're starting from the top (the actual results), and then we'll dig a bit into how you can arrive at these results.\n",
    "\n",
    "The COCO challenge includes a particular way of evaluating the predictions, and there's an API available at its [cocoapi](https://github.com/cocodataset/cocoapi) GitHub repository. The code isn't quite friendly, so we'll be using [Torchvision's reference scripts for object detection](https://github.com/pytorch/vision/tree/main/references/detection) instead.\n",
    "\n",
    "The commands below retrieve the necessary files so we can run the evaluation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526ec6a3",
   "metadata": {
    "id": "526ec6a3"
   },
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/pytorch/vision/v0.15.2/references/detection/coco_eval.py\n",
    "# !wget https://raw.githubusercontent.com/pytorch/vision/v0.15.2/references/detection/transforms.py\n",
    "# !wget https://raw.githubusercontent.com/pytorch/vision/v0.15.2/references/detection/utils.py && mv utils.py detection_utils.py\n",
    "# !wget https://raw.githubusercontent.com/pytorch/vision/v0.15.2/references/detection/engine.py\n",
    "# !wget https://raw.githubusercontent.com/pytorch/vision/v0.15.2/references/detection/coco_utils.py\n",
    "# !sed -i 's/import utils/import detection_utils as utils/' engine.py\n",
    "# !sed -i 's/import utils/import detection_utils as utils/' coco_eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e8eafb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c7e8eafb",
    "outputId": "5bf7d29b-cabc-41a4-a76f-b754cb062f27"
   },
   "outputs": [],
   "source": [
    "from engine import evaluate as torch_coco_eval\n",
    "torch_coco_eval(model, dataloaders['val'], device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0556efb7",
   "metadata": {
    "id": "0556efb7"
   },
   "source": [
    "Let's focus on the two first results for Average Precision. The first one (IoU=0.50:0.95) is the key metric of the COCO challenge, and the second one (IoU=0.50) is the key metric of the Pascal VOC challenge. The latter is easier to understand, so let's start with that one.\n",
    "\n",
    "Its value, 0.608 means that model detect the boxes correctly with a precision of 60.8%, that is, from all the detect boxes, almost two thirds were considered correct. How do we know if a box is correctly detected or not? What about checking the overlap between the true (annotated) box and the predicted one? That's the idea behind the \"IoU\" metric (the intersection over union) we'll discuss now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eafc05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
