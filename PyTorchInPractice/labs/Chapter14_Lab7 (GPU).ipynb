{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11584a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers evaluate portalocker chromadb sentence-transformers rouge-score sec-edgar-downloader langchain xformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb21ba19",
   "metadata": {
    "id": "cb21ba19"
   },
   "source": [
    "## 14.4 Lab 7 / Case 7: Document Q&A\n",
    "\n",
    "In this lab, we'll put together several tools we already used to extract information from a set of documents, also called \"Document Q&A\". We'll retrieve the latest 10-K forms filed by S&P500 top companies and search for information about their reported risks using natural language.\n",
    "\n",
    "Here are the tickers for the top 25 companies, as of June 2023:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70afb004",
   "metadata": {
    "id": "70afb004"
   },
   "outputs": [],
   "source": [
    "tickers = ['AAPL', 'MSFT', 'AMZN', 'NVDA', 'GOOGL', 'GOOG', 'META', 'BRK.B', 'TSLA', 'UNH', 'XOM', 'JPM',\n",
    "           'JNJ', 'V', 'LLY', 'PG', 'AVGO', 'MA', 'HD', 'MRK', 'CVX', 'PEP', 'ABBV', 'KO', 'COST']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb6e4b0",
   "metadata": {
    "id": "ebb6e4b0"
   },
   "source": [
    "### 14.4.1 EDGAR\n",
    "\n",
    "EDGAR is the Securities and Exchange Commission's (SEC) Eletronic Data Gathering, Analysis, and Retrieval (EDGAR) system.\n",
    "\n",
    "\"_[it] performs automated collection, validation, indexing, acceptance, and forwarding of submissions by companies and others who are required by law to file forms with the U.S. Securities and Exchange Commission (SEC). Its primary purpose is to increase the efficiency and fairness of the securities market for the benefit of investors, corporations, and the economy by accelerating the receipt, acceptance, dissemination, and analysis of time-sensitive corporate information filed with the agency._\"\n",
    "\n",
    "Source: [Important Information About EDGAR](https://www.sec.gov/edgar/searchedgar/aboutedgar.htm)\n",
    "\n",
    "### 14.4.2 Form 10-K\n",
    "\n",
    "In this lab, we'll be retrieving the latest 10-K form filed by the companies previously listed.\n",
    "\n",
    "\"_A Form 10-K is an annual report required by the U.S. Securities and Exchange Commission (SEC), that gives a comprehensive summary of a company's financial performance. Although similarly named, the annual report on Form 10-K is distinct from the often glossy \"annual report to shareholders,\" which a company must send to its shareholders when it holds an annual meeting to elect directors (though some companies combine the annual report and the 10-K into one document). The 10-K includes information such as company history, organizational structure, executive compensation, equity, subsidiaries, and audited financial statements, among other information._\"\n",
    "\n",
    "Source: [Wikipedia](https://en.wikipedia.org/wiki/Form_10-K)\n",
    "\n",
    "We'll be paying special attention to the section [\"Item 1A - Risk Factors\"](https://en.wikipedia.org/wiki/Form_10-K#Item_1A_%E2%80%93_Risk_Factors), where \"_...the company lays anything that could go wrong, likely external effects, possible future failures to meet obligations, and other risks disclosed to adequately warn investors and potential investors._\"\n",
    "\n",
    "### 14.4.3 Downloader\n",
    "\n",
    "While it's possible to retrieve public information directly from EDGAR, you'd have to find the proper identification numbers of companies and filings to download the reports. It is more conveniente to use a Python package that handles the nitty-gritty details for us and retrieves as many reports as we want by simply specifying the company's ticker (e.g. MSFT, GOOGL), and the type of report (e.g. 10-K). The package [`sec-edgar-downloader`](https://github.com/jadchaar/sec-edgar-downloader) does exactly that.\n",
    "\n",
    "We can easily download the forms by creating an instance of a `Downloader` that points to the destination folder where files will be stored, and calling its `get()` method repeatedly, once for each ticker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae72256",
   "metadata": {
    "id": "eae72256"
   },
   "outputs": [],
   "source": [
    "from sec_edgar_downloader import Downloader\n",
    "\n",
    "dest_folder = \"./edgar10k_sp500_top25\"\n",
    "dl = Downloader(dest_folder)\n",
    "\n",
    "form = '10-K'\n",
    "for ticker in tickers:\n",
    "    dl.get(\"10-K\", ticker, amount=1, download_details=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477ac167",
   "metadata": {
    "id": "477ac167"
   },
   "source": [
    "Alternatively, you can download the compressed folder containing all forms (as of June 2023):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea4b3dc",
   "metadata": {
    "id": "cea4b3dc"
   },
   "outputs": [],
   "source": [
    "#!wget https://github.com/dvgodoy/assets/releases/download/dataset/edgar10k_sp500_top25.tar.gz\n",
    "#!tar -xvzf edgar10k_sp500_top25.tar.gz\n",
    "#!mv filings edgar10k_sp500_top25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3938a81",
   "metadata": {
    "id": "a3938a81"
   },
   "source": [
    "It will create a subfolder for each ticker, each containing a folder corresponding to the downloaded form (10-K), and yet another folder named after the form's corresponding ID number. That last folder has two files: `filing-details.html` and `full-submission.txt`. We'll be reading the details file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff7e4e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bff7e4e4",
    "outputId": "ab9a7ab1-b376-4a8d-ceb5-7f8ae53a8076"
   },
   "outputs": [],
   "source": [
    "!ls -l edgar10k_sp500_top25/sec-edgar-filings/MSFT/10-K/0001564590-22-026876"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb57c2e3",
   "metadata": {
    "id": "bb57c2e3"
   },
   "source": [
    "### 14.4.4 Parser\n",
    "\n",
    "The details file is a mix of HTML and XML tags, and it would be very cumbersome to parse them ourselves. Fortunately, we can easily adapt a parser function, [`parse_10k_filing()`](https://github.com/rsljr/edgarParser/blob/master/parse_10K.py) from the [edgarParser](https://github.com/rsljr/edgarParser) repository, to parse our downloaded files.\n",
    "\n",
    "Its original docstring states:\n",
    "\n",
    "\"_The function *parse_10k_filing()* parses 10-K forms to extract the following sections: business description, business risk, and management discussioin and analysis. The function takes two arguments, a link and a number indicating the section, and returns a list with the requested sections. Current options are **0(All), 1(Business), 2(Risk), 4(MDA).**_\"\n",
    "\n",
    "We'll be using option number two to retrieve text related to section \"Item 1A - Risk Factors\" only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd391a9",
   "metadata": {
    "id": "dfd391a9"
   },
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/rsljr/edgarParser/blob/master/parse_10K.py\n",
    "import re\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "\n",
    "def parse_10k_filing(content, section):\n",
    "\n",
    "    if section not in [0, 1, 2, 3]:\n",
    "        print(\"Not a valid section\")\n",
    "        sys.exit()\n",
    "\n",
    "    def get_text(content):\n",
    "        html = bs(content, \"html.parser\")\n",
    "        text = html.get_text()\n",
    "        text = unicodedata.normalize(\"NFKD\", text).encode('ascii', 'ignore').decode('utf8')\n",
    "        text = text.split(\"\\n\")\n",
    "        text = \" \".join(text)\n",
    "        return(text)\n",
    "\n",
    "    def extract_text(text, item_start, item_end):\n",
    "        item_start = item_start\n",
    "        item_end = item_end\n",
    "        starts = [i.start() for i in item_start.finditer(text)]\n",
    "        ends = [i.start() for i in item_end.finditer(text)]\n",
    "        positions = list()\n",
    "        for s in starts:\n",
    "            control = 0\n",
    "            for e in ends:\n",
    "                if control == 0:\n",
    "                    if s < e:\n",
    "                        control = 1\n",
    "                        positions.append([s,e])\n",
    "        item_length = 0\n",
    "        item_position = list()\n",
    "        for p in positions:\n",
    "            if (p[1]-p[0]) > item_length:\n",
    "                item_length = p[1]-p[0]\n",
    "                item_position = p\n",
    "\n",
    "        item_text = text[item_position[0]:item_position[1]]\n",
    "\n",
    "        return(item_text)\n",
    "\n",
    "    text = get_text(content)\n",
    "\n",
    "    if section == 1 or section == 0:\n",
    "        try:\n",
    "            item1_start = re.compile(\"item\\s*[1][\\.\\;\\:\\-\\_]*\\s*\\\\b\", re.IGNORECASE)\n",
    "            item1_end = re.compile(\"item\\s*1a[\\.\\;\\:\\-\\_]\\s*Risk|item\\s*2[\\.\\,\\;\\:\\-\\_]\\s*Prop\", re.IGNORECASE)\n",
    "            businessText = extract_text(text, item1_start, item1_end)\n",
    "        except:\n",
    "            businessText = \"Something went wrong!\"\n",
    "\n",
    "    if section == 2 or section == 0:\n",
    "        try:\n",
    "            item1a_start = re.compile(\"(?<!,\\s)item\\s*1a[\\.\\;\\:\\-\\_]\\s*Risk\", re.IGNORECASE)\n",
    "            item1a_end = re.compile(\"item\\s*2[\\.\\;\\:\\-\\_]\\s*Prop|item\\s*[1][\\.\\;\\:\\-\\_]*\\s*\\\\b\", re.IGNORECASE)\n",
    "            riskText = extract_text(text, item1a_start, item1a_end)\n",
    "        except:\n",
    "            riskText = \"Something went wrong!\"\n",
    "\n",
    "    if section == 3 or section == 0:\n",
    "        try:\n",
    "            item7_start = re.compile(\"item\\s*[7][\\.\\;\\:\\-\\_]*\\s*\\\\bM\", re.IGNORECASE)\n",
    "            item7_end = re.compile(\"item\\s*7a[\\.\\;\\:\\-\\_]\\sQuanti|item\\s*8[\\.\\,\\;\\:\\-\\_]\\s*\", re.IGNORECASE)\n",
    "            mdaText = extract_text(text, item7_start, item7_end)\n",
    "        except:\n",
    "            mdaText = \"Something went wrong!\"\n",
    "\n",
    "    if section == 0:\n",
    "        data = [businessText, riskText, mdaText]\n",
    "    elif section == 1:\n",
    "        data = [businessText]\n",
    "    elif section == 2:\n",
    "        data = [riskText]\n",
    "    elif section == 3:\n",
    "        data = [mdaText]\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcc2b46",
   "metadata": {
    "id": "8fcc2b46"
   },
   "source": [
    "Let's parse the latest 10-K form filed by Microsoft (as of June 2023):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f60315",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50f60315",
    "outputId": "a4906ec9-1a81-493c-b7e1-83b931521df8"
   },
   "outputs": [],
   "source": [
    "with open('./edgar10k_sp500_top25/sec-edgar-filings/MSFT/10-K/0001564590-22-026876/filing-details.html', 'r',\n",
    "          encoding='utf-8') as f:\n",
    "    html = f.read()\n",
    "\n",
    "res = parse_10k_filing(html, 2)[0]\n",
    "len(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51585ffc",
   "metadata": {
    "id": "51585ffc"
   },
   "source": [
    "That's about 70,000 characters. We need to split it into more manageable chunks.\n",
    "\n",
    "### 14.4.5 Chunking Strategies\n",
    "\n",
    "There is no right or wrong answer to how you should split the text into chunks. It depends on a series of factors such as the type of text you're dealing with (long reports or short tweets, for example), the model you're using to embed the text and the nature of your queries (more on that later), and limitations in size (models typically have a maximum input length as we've already seen).\n",
    "\n",
    "For more details, check the [\"Chunking Strategies for LLM Applications\"](https://www.pinecone.io/learn/chunking-strategies/) blog post.\n",
    "\n",
    "Having said that, it's possible to chunk your text using a fixed-length or a content-aware approach. Let's take a quick look at some of them.\n",
    "\n",
    "#### 14.4.5.1 Fixed-Length\n",
    "\n",
    "Fixed-length approaches split the text into equal-length chunks (e.g. 300 characters/words/tokens) with or without some overlap between them. [Langchain](https://github.com/hwchase17/langchain), a Python package that has grown a lot in popularity in the last few months, and that allows you to integrate different tools (e.g. embedding models, vector databases) into a workflow, also offers a convenient way of [splitting text into chunks](https://python.langchain.com/docs/modules/data_connection/document_transformers/#get-started-with-text-splitters) of fixed-length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb5aacb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eeb5aacb",
    "outputId": "2fd59ac1-dfb2-4519-b56e-65172155e461"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=20)\n",
    "docs = text_splitter.create_documents([res])\n",
    "docs[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f607097",
   "metadata": {
    "id": "4f607097"
   },
   "source": [
    "#### 14.4.5.2 Content-Aware\n",
    "\n",
    "The problem with fixed-length is that the chunks will most certainly end mid-sentence. The overlap may help mitigate this issue, but it won't work for long sentences. That's when the content-aware approach comes in. We can split it by sentences or paragraphs using indications in the text's structure. We could, for example, naively split the text into sentences using the period (.) as an indication of the end of a sentence. What about exclamation and question marks?\n",
    "\n",
    "Fortunately, sentence tokenizing is a very well-known problem, and the traditional [Natural Language Toolkit (NLTK)](https://www.nltk.org/) package has a sentence tokenizer available. We only need to download the `punkt` tokenizer package and it will be ready to be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1344c93e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1344c93e",
    "outputId": "3a1c2fbb-6e0c-450b-88da-160a636407ca"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eade466d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eade466d",
    "outputId": "e443e851-2ace-4a27-9556-23be30dfd349"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "docs = sent_tokenize(res)\n",
    "docs[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55a5898",
   "metadata": {
    "id": "b55a5898"
   },
   "source": [
    "As you can see, each element in the list of documents is a single sentence.\n",
    "\n",
    "Sometimes, as in the case of our 10-K form filed by Microsoft, there's some other indication to the text's structure: it looks like paragraphs are separated by a sequence of two or more spaces.\n",
    "\n",
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e8137f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67e8137f",
    "outputId": "2e0c5e10-b611-4856-d5d1-843b012939f2"
   },
   "outputs": [],
   "source": [
    "docs = res.split('  ')\n",
    "docs[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46797ac",
   "metadata": {
    "id": "e46797ac"
   },
   "source": [
    "Looks good, these are definitely paragraphs. Unfortunately, this may not be the case for every document: in some 10-K forms, there's no clear indication of a paragraph, and you'll need to rely on a different chunking strategy to move forward. For now, we're sticking with this particular 10-K form, and we'll proceed using paragraphs as chunks.\n",
    "\n",
    "If we look at the full list, though, we'll see that there are many empty lines as well as really short ones that are likely section headers. We can discard these chunks that are too short (say, less than 10 characters long)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccd72c9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ccd72c9",
    "outputId": "ffdd725b-c510-410d-b8c6-81c688027fcd"
   },
   "outputs": [],
   "source": [
    "paragraphs = list(map(lambda s: s.strip(), filter(lambda s: len(s) > 10, res.split('  '))))\n",
    "len(paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd4242c",
   "metadata": {
    "id": "7bd4242c"
   },
   "source": [
    "We got 88 paragraphs. Let's take a look at one of those paragraphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889a0db1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "889a0db1",
    "outputId": "f6b0da2a-8d65-4d36-b6c1-5e0cfe280d8b"
   },
   "outputs": [],
   "source": [
    "text = paragraphs[1]\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bd78d8",
   "metadata": {
    "id": "d1bd78d8"
   },
   "source": [
    "How many words is that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b89f58b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8b89f58b",
    "outputId": "a5f93355-6025-42ab-e8d0-551dcbc1b45e"
   },
   "outputs": [],
   "source": [
    "len(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ce9b6d",
   "metadata": {
    "id": "82ce9b6d"
   },
   "source": [
    "Perhaps we can make it shorter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d124c7f6",
   "metadata": {
    "id": "d124c7f6"
   },
   "source": [
    "### 14.4.6 Summarization\n",
    "\n",
    "Load a pretrained summarization pipeline from HuggingFace and use it to summarize the text above. Try different minimum and maximum lengths and observe the resulting summaries. How they compare to the original text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68341371",
   "metadata": {
    "id": "68341371"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "summarizer = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da40f2e1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "da40f2e1",
    "outputId": "a4742d22-3c03-40ea-b944-ea249d9887e0"
   },
   "outputs": [],
   "source": [
    "summarizer(text, max_length=50, min_length=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff65b3b",
   "metadata": {
    "id": "bff65b3b"
   },
   "source": [
    "Summarizing text is great, but we may be doing it prematurely at this point. Instead of summarizing individual paragraphs (or other chunks of text), it may be more interesting to find (full) paragraphs of interest first, and only then summarize them as a whole.\n",
    "\n",
    "If we're doing document Q&A, we need to query our documents (paragraphs) and find those that are more likely to contain the answer, that is, those more closely related to the topic of our query.\n",
    "\n",
    "How can we search for similar documents?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd5cf1f",
   "metadata": {
    "id": "3dd5cf1f"
   },
   "source": [
    "### 14.4.7 Embeddings\n",
    "\n",
    "You already know how to search for similar documents. You need to embed them first!\n",
    "\n",
    "Use the `sentence transformers` package to load a pretrained model for sentence embeddings (e.g. `all-MiniLM-L12-v2`) and embed every paragraph of text from Microsoft's 10-K form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb300be",
   "metadata": {
    "id": "ffb300be"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2790dba4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2790dba4",
    "outputId": "bbeab982-1abc-4d32-98d7-4fb65e2989e4"
   },
   "outputs": [],
   "source": [
    "embeddings = ...\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648b7cce",
   "metadata": {
    "id": "648b7cce"
   },
   "source": [
    "### 14.4.8 Searching\n",
    "\n",
    "There are two alternatives to search for similar embeddings, and we have tried them both already: PyTorch's own cosine similarity, and vector databases such as FAISS or ChromaDB.\n",
    "\n",
    "At this point, let's keep it as simple as it can be, and stick with cosine similarity. Create an instance of the cosine similarity layer and use it to find five paragraphs that are most similar to the query below (don't forget to embed the query as well):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3194e9c7",
   "metadata": {
    "id": "3194e9c7"
   },
   "outputs": [],
   "source": [
    "query = \"what are the sources of uncertainties?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a14d98",
   "metadata": {
    "id": "c9a14d98"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "similarity = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f455a7",
   "metadata": {
    "id": "44f455a7"
   },
   "outputs": [],
   "source": [
    "# Embed the query and make it a tensor\n",
    "q = ...\n",
    "content = torch.as_tensor(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3240bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0c3240bf",
    "outputId": "d01284c8-ba89-40f8-f3e5-d7070ba8e4df"
   },
   "outputs": [],
   "source": [
    "# Compute the cosine similarity between query and content\n",
    "# and get the top 5 results\n",
    "similarities = ...\n",
    "most = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192342d8",
   "metadata": {
    "id": "192342d8"
   },
   "source": [
    "You should get a list of five indices corresponding to the paragraphs that are most relevant to our query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c2bca1",
   "metadata": {
    "id": "00c2bca1"
   },
   "source": [
    "### 14.4.9 Context\n",
    "\n",
    "Now, join all the paragraphs together as a single piece of text. This is going to be what is referred to as the \"context\". Notice that the indices may be ordered according to their similarity to the query. However, it's probably a good idea to order them as they appear on the text instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6789a43f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6789a43f",
    "outputId": "2d238c72-c222-4a1e-8f0c-47f8fef1df24"
   },
   "outputs": [],
   "source": [
    "context = ...\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2db4a7b",
   "metadata": {
    "id": "f2db4a7b"
   },
   "source": [
    "The context should contain the relevant information to answer our query, and it is one of the arguments you need to pass to a question answering pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e0162d",
   "metadata": {
    "id": "c1e0162d"
   },
   "source": [
    "### 14.4.10 Question Answering\n",
    "\n",
    "We have a question, \"_what are the sources of uncertainties?_\" and we have a context, five paragraphs from our text that are the most similar to the question. That's everything you need to try a question answering pipeline!\n",
    "\n",
    "Create an instance of Q&A pipeline, and call it using its `question` and `context` arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02e8895",
   "metadata": {
    "id": "a02e8895",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "qa_model = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e90233",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a9e90233",
    "outputId": "e08310a6-a3b1-42e2-aa7b-f1e171c1a670"
   },
   "outputs": [],
   "source": [
    "query = \"what are the sources of uncertainties?\"\n",
    "\n",
    "qa_model(question=query, context=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4482b5d8",
   "metadata": {
    "id": "4482b5d8"
   },
   "source": [
    "The Q&A model is good at answering questions that are extractive in nature and can be easily pinpointed in the text. It gives you back the start and end positions in the text that contain the answer to your question.\n",
    "\n",
    "It may technically correct, but perhaps it's a bit too short, right?\n",
    "\n",
    "In theory, the context should contain the relevant information to our query. But, it is too verbose and it doesn't read well, after all, it is just a sequence of paragraphs patched together. One way of trying to make it look more like an answer is to summarize it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3919d3",
   "metadata": {
    "id": "8a3919d3"
   },
   "source": [
    "Use the summarization pipeline you already created to summarize the context above. Make sure the minimum and maximum length are appropriate given the original length of the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf8aeb8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2cf8aeb8",
    "outputId": "1dd84ba2-a38d-4dcf-c42f-0806f516306d"
   },
   "outputs": [],
   "source": [
    "summary = ...\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e161b723",
   "metadata": {
    "id": "e161b723"
   },
   "source": [
    "How do you like the summary? Does it look like an answer to our question? Could it have been better? Pause and ponder for a while, what could you do to get a better answer or, better yet, to get a better context back?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14568521",
   "metadata": {
    "id": "14568521"
   },
   "source": [
    "### 14.4.11 Hallucinations\n",
    "\n",
    "Hallucinations have a bad reputation. The term refers to generated text that's completely fictional while being portrayed as factual in an answer given by a generative model such as ChatGPT.\n",
    "\n",
    "But hallucinations may be useful if handled with care. You see, one of the issues with the Q&A is that we're searching for paragraphs that are most similar to our question. Wouldn't make more sense to search for paragraphs that are most similar to an answer instead? Well, yes, but if you knew the answer already, you wouldn't need to ask the question, right?\n",
    "\n",
    "But no one said it had to be a real or factual answer! We may very well use a fake, made-up, completely hallucinated answer instead. If it looks like an answer, it sounds like an answer, and it has the structure of an answer, it IS an answer as far as semantic search is concerned.\n",
    "\n",
    "Our original question was \"_what are the sources of uncertainties?_\", and we're looking for an answer in 10-K filings of S&P 500 companies, Microsoft in this case. So, what if we rephrase our question as \"_what uncertainties are likely faced by an S&P500 company?_\" and ask a Large Language Model to come up with an answer?\n",
    "\n",
    "We'll be doing that later in the course but, for now, assume that the answer generated by the LLM is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa3862f",
   "metadata": {
    "id": "ffa3862f"
   },
   "outputs": [],
   "source": [
    "hallucinated_answer = \"\"\"\n",
    "    As an S&P 500 company, uncertainies such as market fluctuations, changes in customer demand,\n",
    "    and new competitors entering the market can be challenging to handle. Additionally, companies\n",
    "    must also be prepared to adapt to changing trends and technological innovations, while ensuring\n",
    "    that their products and services remain competitive and high-quality. Furthermore, companies\n",
    "    must carefully evaluate their target market to remain relevant and generate revenue.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd559af9",
   "metadata": {
    "id": "dd559af9"
   },
   "source": [
    "It surely does look like an answer, right? Now, make this made-up answer your \"question\" and:\n",
    "- get its embeddings\n",
    "- use the embeddings to search for similar paragraphs in the text\n",
    "- use the search results to build a context\n",
    "- summarize the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa063627",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "aa063627",
    "outputId": "7107389c-d993-4d3b-cc20-6683d28ceed9"
   },
   "outputs": [],
   "source": [
    "# Embed the hallucinated answer and make it a tensor\n",
    "q = ...\n",
    "\n",
    "# Compute the cosine similarity between query and content\n",
    "# and get the top 5 results\n",
    "similarities = ...\n",
    "most = ...\n",
    "\n",
    "# Concatenate the corresponding paragraphs together\n",
    "hallucination_context = ...\n",
    "hallucination_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91c9e86",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d91c9e86",
    "outputId": "3ed32c31-08c6-49ae-ddff-1ea1d16ce9d0"
   },
   "outputs": [],
   "source": [
    "hallucination_summary = summarizer(hallucination_context, min_length=100, max_length=250)\n",
    "hallucination_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cafba1a",
   "metadata": {
    "id": "1cafba1a"
   },
   "source": [
    "How do you like the new summary? Is it better than the previous one?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedbc05b",
   "metadata": {
    "id": "bedbc05b"
   },
   "source": [
    "### 14.4.12 Asymmetric Semantic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57ee459",
   "metadata": {
    "id": "c57ee459"
   },
   "source": [
    "If you think that resorting to \"hallucinated\" answers may be impractical, there's an alternative: asymmetric semantic search. As it turns out, the model we've been using to embed our sentences and paragraphs is better suited for assessing similarities of sentences and paragraphs of similar length. It worked well to look for nuclear plant-related news in the AG News Dataset, and it is a good fit for clustering pieces of text together for example.\\\n",
    "\n",
    "However, as we already pointed out, our question is much shorter than any of the paragraphs, a typical case of asymmetric semantic search that asks for a different model to produce embeddings. For more details about recommended models for both [symmetric](https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models) and [asymmetric](https://www.sbert.net/docs/pretrained-models/msmarco-v3.html) search, check the [documentation] of sentence transformers.\n",
    "\n",
    "In the `sentence_transformers` package, pretrained models for asymmetric search are based on [MSMARCO](https://microsoft.github.io/msmarco/), \"_a large scale information retrieval corpus that was created based on real user search queries using Bing search engine._\" Let's create an instance of one of these models.\n",
    "\n",
    "Choose a model from the list of [asymmetric](https://www.sbert.net/docs/pretrained-models/msmarco-v3.html) models, and make sure you pick one that's fine-tuned for cosine similarity, since it is the similarity metric we're using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0424ad17",
   "metadata": {
    "id": "0424ad17"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "asymmetric_model = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1294d2",
   "metadata": {
    "id": "2c1294d2"
   },
   "source": [
    "Remember that you cannot mix and match embeddings, so you need to embed the paragraphs once again using the new model first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c216e0",
   "metadata": {
    "id": "f7c216e0"
   },
   "outputs": [],
   "source": [
    "asymmetric_embeddings = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a45f5c",
   "metadata": {
    "id": "d9a45f5c"
   },
   "source": [
    "So far, you've been using PyTorch's cosine similarity layer and its `topk()` function to retrieve the most similar documents. Now, you will use Sentence Transformers' own [`semantic_search()`](https://www.sbert.net/examples/applications/semantic-search/README.html#util-semantic-search) helper function. It takes the following arguments:\n",
    "- `query_embeddings`: your embedded question\n",
    "- `corpus embeddings`: your embedded context\n",
    "- `top_k`: how many results it should return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bf5e16",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7bf5e16",
    "outputId": "be27ebd2-f2df-4b2d-a660-9ebb8b55792e"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers.util import semantic_search\n",
    "\n",
    "query = \"what are the sources of uncertainties?\"\n",
    "# Get the embeddings for the query above using the asymmetric model\n",
    "asymmetric_query = ...\n",
    "\n",
    "# Use the semantic_search helper function to get a list of the top 5 results\n",
    "similarities = ...\n",
    "similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc73c464",
   "metadata": {
    "id": "bc73c464"
   },
   "source": [
    "Notice that each result is a dictionary, so you have to unpack the paragraph indices (`corpus_id`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb7683c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fcb7683c",
    "outputId": "cd43bcec-79f3-42d1-9d55-189cf033dab1"
   },
   "outputs": [],
   "source": [
    "most = [s['corpus_id'] for s in similarities]\n",
    "most"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2766d716",
   "metadata": {
    "id": "2766d716"
   },
   "source": [
    "Now, it is business as usual: assemble the context by joining the matched paragraphs and submit it to the summarization pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3122e4b8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "3122e4b8",
    "outputId": "e48afac1-6b25-4afd-fa97-8360627661df"
   },
   "outputs": [],
   "source": [
    "asymmetric_context = ...\n",
    "asymmetric_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e3e407",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "id": "06e3e407",
    "outputId": "0c075cf1-ccc9-4022-c21b-b99dabbacfb2"
   },
   "outputs": [],
   "source": [
    "# We're moving the summarize to the CPU for this, please don't remove this code\n",
    "device = summarizer.device\n",
    "summarizer.model.to('cpu')\n",
    "summarizer.device = torch.device('cpu')\n",
    "\n",
    "asymmetric_summary = summarizer(asymmetric_context, min_length=100, max_length=250)\n",
    "asymmetric_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc884755",
   "metadata": {
    "id": "bc884755"
   },
   "source": [
    "Did you get an error by any chance? Good! It means that your context is too long for the summarization pipeline you're using. This information is available at the configuration of the model that powers the pipeline, and you can retrieve it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572b7e6a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "572b7e6a",
    "outputId": "04e06d66-ea6a-4a3c-ba6e-45fad9562de0"
   },
   "outputs": [],
   "source": [
    "max_len = summarizer.model.config.max_position_embeddings\n",
    "max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45188511",
   "metadata": {
    "id": "45188511"
   },
   "source": [
    "Now, you have to double-check how long (in tokens) your context is. Remember that there's usually more tokens than words in a document. The summaritzation pipeline also contains the tokenizer it uses to preprocess its inputs. Do you remember which method from the tokenizer can be used to encode a sentence into the corresponding list of token indices? Use this method get a list of token indices for your context, and see how long it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b74ce9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d0b74ce9",
    "outputId": "64852d17-a905-4f7c-e906-2efdf768a54c"
   },
   "outputs": [],
   "source": [
    "# Tip: set the argument `add_special_tokens=False` to count the real number of tokens\n",
    "token_list = ...\n",
    "len(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81be4e10",
   "metadata": {
    "id": "81be4e10"
   },
   "source": [
    "For curiosity's sake, let's compare the number of words with the number of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63f31fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e63f31fd",
    "outputId": "72bc1fca-e16f-4e3d-f08b-d19778552707"
   },
   "outputs": [],
   "source": [
    "len(asymmetric_context.split())/len(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4f4a26",
   "metadata": {
    "id": "4c4f4a26"
   },
   "source": [
    "The ratio should be around 0.8, meaning that 1,000 tokens roughly correspond to 800 words. That's the rule of thumb you can use if you're using a paid API since they charge by number of tokens, not words.\n",
    "\n",
    "#### 14.4.12.1 Trimmed Context\n",
    "\n",
    "Back to our issue, if the number of tokens in your context is higher than what the model can take (1,024 in our case), you need to make the context shorter. One alternative is to simply trim it at exactly 1,022 tokens, that's two less than the maximum length to account for the special tokens at both start and end of the input. You can trim the token list itself and use the tokenizer's `decode()` method to turn it back into text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c616fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "80c616fe",
    "outputId": "8bd08642-228e-4c91-fc98-8952838f1eef"
   },
   "outputs": [],
   "source": [
    "trimmed_context = summarizer.tokenizer.decode(token_list[:(max_len-2)])\n",
    "trimmed_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720659a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "720659a4",
    "outputId": "75281c93-316e-4cc8-c2e3-27b9fa6ef33f"
   },
   "outputs": [],
   "source": [
    "# We can bring the summarizer back to the GPU now\n",
    "summarizer.model.to('cuda')\n",
    "summarizer.device = torch.device('cuda:0')\n",
    "\n",
    "trimmed_summary = summarizer(trimmed_context, min_length=100, max_length=250)\n",
    "trimmed_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8150ec",
   "metadata": {
    "id": "4a8150ec"
   },
   "source": [
    "#### 14.4.12.2 Unsorted Context\n",
    "\n",
    "Until now, we've been sorting the matched documents by their indices to preserve the original flow of the text before summarizing. Trimming the context, however, may lead to the most relevant parts being removed from it, so it may be worth trying to keep the documents sorted by their decreasing matching score instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18a3661",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "b18a3661",
    "outputId": "3f207852-b592-424d-b735-5ad0b8ac8d5c"
   },
   "outputs": [],
   "source": [
    "unsorted_context = '\\n'.join([paragraphs[i] for i in most])\n",
    "unsorted_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbdc2f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0fbdc2f4",
    "outputId": "112e8743-9392-4624-9db1-fe6ee577c29d"
   },
   "outputs": [],
   "source": [
    "# Get the token list of the unsorted context above\n",
    "token_list = ...\n",
    "# Trim the token list and use the `decode()` method to get text back\n",
    "trimmed_unsorted_context = ...\n",
    "trimmed_unsorted_summary = summarizer(trimmed_unsorted_context, min_length=100, max_length=250)\n",
    "trimmed_unsorted_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108ef270",
   "metadata": {
    "id": "108ef270"
   },
   "source": [
    "### 14.4.13 ROUGE Score\n",
    "\n",
    "How can you tell if the summary is good or not? ROUGE, which stands for Recall-Oriented Understudy for Gist Evaluation, is a metric used to compare an automatically produced summary against a reference (a human-produced high quality summary). We're not going into its details here, but the general idea is to compute precision and recall for matching N-grams. An 1-gram is just a word, a 2-gram is a pair of consecutive words, and so on, and so forth, as illustrated in the figure below:\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch14/ngrams.png)\n",
    "\n",
    "For example, the ROUGE-1 score between \"_nice to meet you here_\" and \"_nice to meet you now_\" is 0.8 precision and 0.8 recall because it got 4 out of 5 words (1-grams) in common. Let's use a Python package, [ROUGE Scourer](https://github.com/google-research/google-research/tree/master/rouge), to try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cd4b73",
   "metadata": {
    "id": "38cd4b73"
   },
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0baa87",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aa0baa87",
    "outputId": "1c75dc0d-3ed8-4dd7-d52a-8e8ae11772b3"
   },
   "outputs": [],
   "source": [
    "scorer.score('nice to meet you here', 'nice to meet you now')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106c82f5",
   "metadata": {
    "id": "106c82f5"
   },
   "source": [
    "For 2-grams, it is 0.75 because it matches 3 out of 4 pairs of words (2-grams): \"_nice to_\", \"_to meet_\", and \"_meet you_\" are matches, but \"_you here_\" and \"_you now_\" aren't. Precision and recall are computed using the number of N-grams in the prediction and the reference/target, respectively, in the denominator.\n",
    "\n",
    "There's also the ROUGE-L metric, which stands for the longest common subsequence (LCS) (not necessarily consecutive, but in the same order) in both texts. ROUGE-L precision is the length of the LCS over the number of unigrams in the prediction, and ROUGE-L recall is the length the LCS over the number of unigrams in the reference/target. In our example, both values are 0.8 because the LCS is four words long (\"_nice to meet you_\") and both sentences are five words long.\n",
    "\n",
    "We don't have a reference or target here because there's no high-quality human-produced summary to compare to. However, we can use the full context as target, and look at the precision metric to have a rough idea of the commonalities between the generated summary and the original text. You'll notice that recall values are quite low, but that's due to the fact that the original text is much longer, and its length is used in the denominator for computing recall metrics in ROUGE.\n",
    "\n",
    "Here are the scores for every pair of context and summary you generated in this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24498237",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24498237",
    "outputId": "cc3d2e7e-a842-4521-c258-96d03b120fd3"
   },
   "outputs": [],
   "source": [
    "scores = scorer.score(context,\n",
    "                      summary[0]['summary_text'].strip())\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bab6f31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1bab6f31",
    "outputId": "46e22546-d9ae-4820-f7e2-3884ef1a85a0"
   },
   "outputs": [],
   "source": [
    "scores = scorer.score(hallucination_context,\n",
    "                      hallucination_summary[0]['summary_text'].strip())\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352b65df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "352b65df",
    "outputId": "85d1afaf-45b8-4dd3-935b-7823f25babb8"
   },
   "outputs": [],
   "source": [
    "scores = scorer.score(trimmed_context,\n",
    "                      trimmed_summary[0]['summary_text'].strip())\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa56dbad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fa56dbad",
    "outputId": "25c8fded-e1b4-47e4-cdda-4f165fb84f76"
   },
   "outputs": [],
   "source": [
    "scores = scorer.score(trimmed_unsorted_context,\n",
    "                      trimmed_unsorted_summary[0]['summary_text'].strip())\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d364da46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
