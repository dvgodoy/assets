{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install portalocker sentencepiece sacremoses transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfb26a63"
   },
   "source": [
    "## 5.3 Lab 3 / Case 3: Classifying Images\n",
    "\n",
    "Now it is YOUR turn to classify some images!\n",
    "\n",
    "First, you will need to choose and load a [model for image classification](https://pytorch.org/vision/stable/models.html#classification) and its corresponding [weights](https://pytorch.org/vision/stable/models.html#table-of-all-available-classification-weights).\n",
    "\n",
    "Don't forget to retrieve the prescribed transformation function or model corresponding to the model you chose. Also, take a look at its size and accuracy, so you have an idea of its performance.\n",
    "\n",
    "TIP: try a very small model (e.g. MobileNet) and a very large model (e.g. VGG) and see how long they take to run inference on your images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3a26e4c6"
   },
   "source": [
    "### 5.3.1 Load Weights\n",
    "\n",
    "Load the weights from the model of your choice into its own object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9473be1a"
   },
   "outputs": [],
   "source": [
    "from torchvision.models import get_weight\n",
    "\n",
    "weights = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94318ce8"
   },
   "source": [
    "### 5.3.2 Load Model\n",
    "\n",
    "Load the model using Torch Hub and the weights you've just loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8febbc63",
    "outputId": "45acf3f3-932e-483e-eb83-db2083c70dd7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "repo = 'pytorch/vision'\n",
    "\n",
    "model = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0a1aa48"
   },
   "source": [
    "### 5.3.3 Extract Metadata\n",
    "\n",
    "Retrieve the categories used to pretrain the model, and the transformation function that should be applied to the input images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bee4103d"
   },
   "outputs": [],
   "source": [
    "categories = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b76bdd06",
    "outputId": "e387fe33-5b34-410d-bf26-12841bad22c0"
   },
   "outputs": [],
   "source": [
    "transforms_fn = ...\n",
    "transforms_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9437af77"
   },
   "source": [
    "Let's inspect the number of parameters and the metrics of the model you chose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2bb75326",
    "outputId": "aa60afe3-a165-43f1-c134-5efd8bbfbf5a"
   },
   "outputs": [],
   "source": [
    "weights.meta['num_params']/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b448d5b",
    "outputId": "8fc35777-693f-454a-d527-711f374271c6"
   },
   "outputs": [],
   "source": [
    "weights.meta['_metrics']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74de8bc5"
   },
   "source": [
    "### 5.3.4 ImageFolder Dataset\n",
    "\n",
    "To make this lab more entertaining and fun, let's build our own dataset of images from scratch! We'll be using PyTorch's `ImageFolder` dataset, which is a very convenient way of building a dataset from a collection of images organized in folders, one for each category.\n",
    "\n",
    "But, first, we need to download some images! Keep in mind that these models are trained on the ImageNet 1K dataset, so we should choose images that fit into one or more of these categories. In the next part of the course, we'll learn how to fine-tune them so we can classify images into new categories.\n",
    "\n",
    "#### 5.3.4.1 ImageNet Dataset\n",
    "\n",
    "Unfortunately the original ImageNet dataset isn't publicly available, only the original URLs to the images were published. If you're a researcher, though, it is possible to request access to versions of this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0a8614b9"
   },
   "source": [
    "#### 5.3.4.2 Downloading and Saving Images\n",
    "\n",
    "The function below, inspired by Nate Raw's [HuggingPics](https://github.com/nateraw/huggingpics) project, uses HuggingFace's experimental search API to retrieve and save images files to the disk. \n",
    "\n",
    "This is just a quick and dirty way of retrieving a small collection of images that fall under the same search term. As long as your images are neatly organized in folder, one folder for each category, you're good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84dcf759"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "def get_image_from_url(url, headers=None):\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    resp.raise_for_status()\n",
    "    img = Image.open(BytesIO(resp.content))\n",
    "    return img\n",
    "\n",
    "def save_images(folder, search_term, count=10):\n",
    "    if not os.path.exists(folder):\n",
    "        os.mkdir(folder)\n",
    "        \n",
    "    SEARCH_URL = \"https://huggingface.co/api/experimental/images/search\"\n",
    "\n",
    "    params = {\"q\": search_term, \"license\": \"public\", \"imageType\": \"photo\", \"count\": count}\n",
    "\n",
    "    resp = requests.get(SEARCH_URL, params=params)\n",
    "    if resp.status_code == 200:\n",
    "        content = resp.json()['value']\n",
    "        urls = [img['thumbnailUrl'] for img in content]\n",
    "\n",
    "        folder = os.path.join(folder, search_term)\n",
    "        if not os.path.exists(folder):\n",
    "            os.mkdir(folder)\n",
    "\n",
    "        i = 0\n",
    "        for url in urls:\n",
    "            try:\n",
    "                img = get_image_from_url(url)\n",
    "                fname = os.path.join(folder, f'{i}.jpg')\n",
    "                img.save(fname)\n",
    "                i += 1\n",
    "            except Exception:\n",
    "                pass\n",
    "        print(f'Retrieved {i} images for {search_term}')\n",
    "    else:\n",
    "        print(f'Failed to retrieve URLs for {search_term}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5a95384"
   },
   "source": [
    "Let's use the function above to fetch images for three existing categories in ImageNet: hedgehogs, ostriches, and armadillos. I chose those animals because I find them funny (and, sadly, raccoons aren't part of the original 1,000 categories). Feel free to choose any other categories!\n",
    "\n",
    "We're saving the images to the `lab3` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f382baf8",
    "outputId": "5d308f44-a7aa-4671-e5f6-40557410c1e0"
   },
   "outputs": [],
   "source": [
    "targets = ['hedgehog', 'ostrich', 'armadillo']\n",
    "\n",
    "for term in targets:\n",
    "    save_images('./lab3', term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6afc452"
   },
   "source": [
    "Inside the top folder, `lab3`, each search term will have its own folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "683b4f0f",
    "outputId": "0eb51b15-0aca-4d72-c4fb-09a133ee61c5"
   },
   "outputs": [],
   "source": [
    "!ls -l ./lab3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9205fb8"
   },
   "source": [
    "Inside each category folder, such as `armadillo`, there will be a collection of sequentially-numbered images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "268816b4",
    "outputId": "0d76091d-339c-41a8-da1d-d41bfdd6c284"
   },
   "outputs": [],
   "source": [
    "!ls -l ./lab3/armadillo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1716099"
   },
   "source": [
    "Now we're set and we can actually create our `ImageFolder` dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35e18802"
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "dataset = ...\n",
    "targets = dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60ea5e15",
    "outputId": "c12201ee-2d9d-4a4e-ab01-0c9231391cfb"
   },
   "outputs": [],
   "source": [
    "dataset, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0f79cd9e"
   },
   "source": [
    "Notice that we can pass the transforms function (or model) as an argument to the dataset, so it outputs preprocessed images out-of-the-box. Moreover, we're reassinging the targets because the `ImageFolder` dataset uses the alphabetically-ordered folders inside the top folder to numerically-encode the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "221c0235",
    "outputId": "72d98931-0363-46ae-8b58-d874cb0b1593"
   },
   "outputs": [],
   "source": [
    "x, y = dataset[0]\n",
    "targets[y], x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ad6eb83"
   },
   "source": [
    "Clearly, this is a preprocessed image of an armadillo since there are no actual pixel values (in the [0, 255] range) in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1912b6da"
   },
   "source": [
    "### 5.3.5 Making Predictions\n",
    "\n",
    "Now, let's use the pretrained model you've already loaded to predict which category the image above belongs to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "99a029b7",
    "outputId": "5a306f80-e453-46b3-e385-0af4f8424ab7"
   },
   "outputs": [],
   "source": [
    "mini_batch = x.unsqueeze(0)\n",
    "mini_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "590e43e3",
    "outputId": "a258fa4e-6c0b-48cc-9260-f948fac8ad48"
   },
   "outputs": [],
   "source": [
    "# The mini-batch above has a single data point\n",
    "# Call the model and get the corresponding predictions(logits)\n",
    "logit = ...\n",
    "\n",
    "# Fetch the index of the largest logit\n",
    "idx = ...\n",
    "\n",
    "# Find the corresponding category\n",
    "categories[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7b294d31"
   },
   "source": [
    "That can't be right, what if we try it one more time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "11ac3256",
    "outputId": "7e1e1ff4-0d37-468f-ff82-5f1d1868336f"
   },
   "outputs": [],
   "source": [
    "# You can either re-run the cell above, or copy and paste it here first, and run this cell instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fc1b47a"
   },
   "source": [
    "It's very likely that, not only you'll get a wrong prediction again, but yet a DIFFERENT wrong prediction. Perhaps you've figured it out that I (purposefully) forgot to set the model to evaluation mode. That shoudl fix it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "749822a5",
    "outputId": "0d895f0a-3141-4e3f-fba8-884117ec187e"
   },
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "# write your code here\n",
    "...\n",
    "\n",
    "\n",
    "# Then find the predicted category as above\n",
    "logit = ...\n",
    "idx = ...\n",
    "categories[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2bac8a5"
   },
   "source": [
    "#### 5.3.5.1 Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4136839"
   },
   "source": [
    "The behavior above is due to the existence of dropout layers. Dropout is probabilistic in nature, that is, it will randomly drop some of the inputs to force the model to learn more than one way to achieve its target, thus working as a regularizer. \n",
    "\n",
    "The idea behind regularization is that, if left unchecked, a model will learn the \"easy way out\" of its problem, so forcing it to work with a random subset of features should reduce overfitting and improve generalization. In other words, the model needs to learn how to handle a distribution of values that is centered at the value the output would have if there was no dropout. That works really well, and many models have dropout layers to make them more robust during training.\n",
    "\n",
    "Let's illustrate this with a dummy model that contains only one dropout layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b64cc402"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "dropping_model = nn.Sequential(nn.Dropout(p=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14f975e9"
   },
   "source": [
    "Now, let's create some random input for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4407ad0f",
    "outputId": "0c44f34f-9610-4648-c43e-6c508cf3b75a"
   },
   "outputs": [],
   "source": [
    "random_input = torch.randn(10)\n",
    "random_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cf3b7092"
   },
   "source": [
    "What happens to these inputs once they go through the dropout model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3d03892",
    "outputId": "f573039e-e2e0-47f3-f5f6-ee41c0eb4425"
   },
   "outputs": [],
   "source": [
    "dropping_model.train()\n",
    "output_train = dropping_model(random_input)\n",
    "output_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f030ae0f"
   },
   "source": [
    "On average, half of the values should have been dropped. Don't forget that dropout is probabilistic, so you may get three, or maybe seven, or perhaps four, or exactly five zeros. If you run it a large number of times, the average number of dropped points should be five (since the probability is 0.5).\n",
    "\n",
    "The problem is, we cannot keep this behavior once you deploy the model, otherwise our users will get different predictions for the same input, as we've just seen above. Thus, dropout does not really drop anything once the model is switched to evaluation mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b068166",
    "outputId": "2040113f-c384-489d-a49c-3b0b7375974d"
   },
   "outputs": [],
   "source": [
    "dropping_model.eval()\n",
    "output_eval = dropping_model(random_input)\n",
    "output_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4db36b2"
   },
   "source": [
    "In evaluation mode, nothing gets dropped!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9c06de70"
   },
   "source": [
    "#### 5.3.5.2 Probabilities\n",
    "\n",
    "We can also use the softmax function to convert logits into probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73ceb101",
    "outputId": "3d602579-0fe1-48e2-fea0-5150cba019a9"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "probabilities = F.softmax(logit, dim=0)\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ed99c86"
   },
   "source": [
    "Or, better yet, use PyTorch's `topk` method to get the top K values only together with their corresponding indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f7eb957e",
    "outputId": "a5bcb6bb-dd4e-4ae0-ee8f-d0cb3594d808"
   },
   "outputs": [],
   "source": [
    "values, indices = torch.topk(probabilities, 1)\n",
    "values, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bf1a9bd4"
   },
   "source": [
    "The target or label is the class corresponding to the index above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "f36cd00f",
    "outputId": "863f5a6a-8e09-43aa-9af8-59a5eb6b55b6"
   },
   "outputs": [],
   "source": [
    "categories[indices[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4a75da0e"
   },
   "source": [
    "In a real-world deployment, though, you won't have the input data neatly assembled as a dataset. You will have to create a mini-batch of user's input data, feed it to the model to get its predicted logits, and then convert them into one or more predictions and probabilities that need to be returned to the user.\n",
    "\n",
    "#### 5.3.5.3 Testing\n",
    "\n",
    "Write a function that takes either an URL or a filepath, a model, its prescribed transformations, and a list of target categories, and returns a list of the top K predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2250f8d7"
   },
   "outputs": [],
   "source": [
    "def predict(path_or_url, model, transforms_fn, categories, topk=1, headers=None):\n",
    "    if path_or_url.startswith('http'):\n",
    "        img = get_image_from_url(path_or_url, headers=headers)\n",
    "    else:\n",
    "        img = Image.open(path_or_url)\n",
    "        \n",
    "    # Apply the transformation to the image\n",
    "    preproc_img = ...\n",
    "    \n",
    "    # If the transformation doesn't return a mini-batch\n",
    "    # We make one ourselves by unsqueezing the first dimension\n",
    "    if len(preproc_img.shape) == 3:\n",
    "        preproc_img = preproc_img.unsqueeze(0)\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    # write your code here\n",
    "    ...\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Make predictions (logits)\n",
    "    pred = model(preproc_img)\n",
    "    \n",
    "    # Compute probabilities out of the predicted logits\n",
    "    # and then get the topk values and indices\n",
    "    probabilities = ...\n",
    "    values, indices = ...\n",
    "    \n",
    "    return [{'label': categories[i], 'value': v.item()} for i, v in zip(indices, values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2b350176"
   },
   "source": [
    "Use the metadata from your model's weights as arguments to the function you wrote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a92afe55",
    "outputId": "cd43cbaa-19d9-4d33-8a4b-f398235d41dd"
   },
   "outputs": [],
   "source": [
    "transforms_fn = ...\n",
    "categories = ...\n",
    "\n",
    "# Call the predict function on an image you download, for example ./lab3/ostrich/0.jpg\n",
    "# write your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cb1f3f8a"
   },
   "source": [
    "Let's make a prediction using an image's URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1af8c712",
    "outputId": "1be56f38-b2dd-4c89-e7ff-7154e8a2cd66"
   },
   "outputs": [],
   "source": [
    "url = 'https://upload.wikimedia.org/wikipedia/commons/c/ce/Daisy_G%C3%A4nsebl%C3%BCmchen_Bellis_perennis_01.jpg'\n",
    "# Complying with Wikimedia User Agent's policy: https://meta.wikimedia.org/wiki/User-Agent_policy\n",
    "headers = {'User-Agent': 'CoolBot/0.0 (https://example.org/coolbot/; coolbot@example.org)'}\n",
    "\n",
    "# Call the predict function on an URL of an image, like the one above\n",
    "# Don't forget to pass the headers as argument\n",
    "# write your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
